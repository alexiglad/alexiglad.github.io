<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://alexiglad.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://alexiglad.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-08T03:45:13+00:00</updated><id>https://alexiglad.github.io/feed.xml</id><title type="html">blank</title><subtitle>Alexi Gladstone&apos;s official website. </subtitle><entry><title type="html">The Different Components of Intelligence</title><link href="https://alexiglad.github.io/blog/2024/intelligence_components/" rel="alternate" type="text/html" title="The Different Components of Intelligence"/><published>2024-09-14T00:00:00+00:00</published><updated>2024-09-14T00:00:00+00:00</updated><id>https://alexiglad.github.io/blog/2024/intelligence_components</id><content type="html" xml:base="https://alexiglad.github.io/blog/2024/intelligence_components/"><![CDATA[<p><em>It’s worth noting that most of this blog is not very scientific and is mostly just a useful abstraction for how to view the different components of intelligence (which is especially useful for thinking about current model capablities or benchmarking). The different aspects of intelligence are abstract ideas, don’t necesssarily represent the underlying way the mind works, and may not encompass all different portions of intelligence. If you come up with other aspects of intelligence please email me :)</em></p> <h3 id="1-memorization-and-knowledge">1) Memorization and Knowledge</h3> <p>Memorization broadly refers to the ability to recite information. Knowledge goes a step beyond this, and involves applying memorized information to new contexts. Knowledge often involves understanding at a deeper level than sole memorization.</p> <p>As of 2024, Large Language Models (LLMs) are pretty good at memorization and, are half decent at knowledge, but fail at most of the other aspects of intelligence discussed below.</p> <h3 id="2-learning-efficiency">2) Learning Efficiency</h3> <p>Because of evolution, it’s easy to see that most modern animals (<a href="https://pubmed.ncbi.nlm.nih.gov/24390479/">and even plants</a>), learn pretty efficiently.</p> <p><a id="image-experiment"></a> </p> <p><img src="/assets/img/blog/dog_experiment.png" alt="object choice task experiment"/></p> <p>As an example of animal learning efficiency, if you put a treat on the floor under a cup, and then put down another cup (with the dog watching), and then point to the cup without a treat under it (<a href="#image-experiment">see example image</a>, <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Flink.springer.com%2F10.1007%2F978-3-319-47829-6_100-1&amp;psig=AOvVaw2cVxlE7-ZVPwfrL25o3WDF&amp;ust=1726449432306000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBcQjhxqFwoTCLDCsbzjw4gDFQAAAAAdAAAAABAE">link</a>), <a href="https://www.psychologytoday.com/us/blog/canine-corner/201502/study-dogs-can-identify-liars-and-they-dont-trust-them">dogs may follow your point</a>. However, often after 1 or a few times doing this dogs will often learn <strong>not</strong> to trust your point. This example demonstrates that dogs will learn quickly given just a few or even 1 example/’shot’/trial.</p> <p>Within the context of AI, the idea of learning efficiency <a href="https://arxiv.org/abs/2205.06743">has long been a focus</a>. People often claim that modern models are great at zero-shot or few-shot learning… but is this really the case?</p> <p>I believe that “Zero-shot” learning (and few-shot similarly) can be interpreted in two distinct ways:</p> <ol> <li> <p><strong>Not seeing something at all during training and being able to predict it on the test set</strong>: E.g. if you were doing supervised classification, not seeing a dog at all during training, but knowing what a dog is during testing. This is a harder problem and, in my opinion, is impossible.</p> </li> <li> <p><strong>Not specifically training or learning for a downstream task but still being able to do the task</strong>: This is relatively easier and has been solved in some cases, such as CLIP for image recognition or LLMs being able to extract entities. These models were not trained for the specific task, yet are still able to perform it. It’s worth noting though, that these models have seen the concepts/tasks they are being tested on throughout pre-training (or some form of pre-training).</p> </li> </ol> <p>The first definition is the literal definition of zero-shot, while the latter definition is the more commonly used and easier version.</p> <p>So How Does Pre-Training Data Affect “Zero-Shot” Performance?</p> <p>The paper “<a href="https://arxiv.org/pdf/2404.04125">Do Multimodal Models Really Achieve Zero-Shot Generalization?</a>” states the following:</p> <blockquote> <p>“We consistently find that, far from exhibiting “zero-shot” generalization, multimodal models require exponentially more data to achieve linear improvements in downstream “zero-shot” performance, following a sample inefficient log-linear scaling trend.”</p> </blockquote> <p>This finding aligns with the second definition of “zero-shot”, and supports the idea that with modern AI we are still far from true zero or few shot generalization/learning efficiency (the first definition).</p> <p>Recent benchmarks, such as <a href="https://arcprize.org/">ARC-AGI</a>, further support this. ARC-AGI measures the abilities of models to learn a completely new task, given just a few examples and then perform that task. Humans can achieve nearly 100% on this benchmark, but modern LLMs only score around 21%, and top approaches score less than 50% (both of these values are based off of when this blog was written in 2024). It’s worth noting that problems in the benchmark get progressively (one could even say exponentially) more difficult, so the jump from 21% to 50% is <strong>very significantly</strong> easier than the jump from 50% to 100%.</p> <details> <summary>If you are now tempted to say, "What about LLMs which are great at few shot learning?"</summary> LLMs are not good at few shot learning unless they have already been trained on data similar to whatever task is being performed. For benchmarks such as ARC, which are completely out of distribution for the pre-training data of LLMs, they do terrible at few shot learning. AI researchers have become accustomed to calling models good at few shot learning, even though models have often seen similar examples during pre-training hundreds or even thousands of times during training. [Paper](https://arxiv.org/pdf/2404.04125) </details> <p><br/></p> <h3 id="3-generalization-creativity-and-invention">3) Generalization, Creativity, and Invention</h3> <p>Generalization, in the context of intelligence, can broadly be defined as “the ability to apply learned knowledge or skills to new and varied situations.” Similarly, creativity can be thought of as “the ability to generate novel and original ideas or solutions by combining existing knowledge or skills in innovative ways<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.” It’s easy to see that the further someone is able to generalize, the better their creativity will likely be. It’s also worth noting that creativity and generalization are spectrums, and not binary skills that intelligent systems either do or don’t have.</p> <p>One cool thing about creativity, is that <em>great</em> creativity can lead to invention.</p> <p>Now, if we were to look at our society now, and compare it to societies from 1,000 years ago, 10,000 years, etc; the differences we’d see could be explained by one thing–invention. Inventions, one after another, have led to the revolutionization of our society. Without invention, humans would be no different than apes.</p> <h4 id="the-most-challenging-aspect-of-intelligence">The Most Challenging Aspect of Intelligence</h4> <p>I’ve ordered this blog based off of which aspects of intelligence I believe to be easiest (memorization, knowledge) to hardest (generalization). Although I don’t think I can argue for this ‘ordering of intelligence attributes’ scientifically, a simple evolutionary argument can be made. Animals, and even plants as we have seen, are capable of memorization and few shot learning. However, their abilities to invent with the same success as humans is limited. As such, we can say that humans have better generalization/creativity than other animals and plants, and that this is something extremely challenging to develop through intelligence (since only humans have it).</p> <h4 id="compression">Compression</h4> <p>Tons of AI researchers love mentioning how compression is linked/core to intelligence (<a href="https://arxiv.org/pdf/2309.10668">paper</a>, <a href="https://x.com/arankomatsuzaki/status/1780073500536872990">twitter mention</a>). I’ve always agreed with this statement. To me, however, it was never clear <em>why</em> (at least with an elegant explanation). I think the framework discussed in this blog to look at different aspects of intelligence provides a great lens, here’s how:</p> <p>Compression in an of itself is an aspect of memorization/knowledge. Let’s say animal A has compressed 1000 bits of information into 100 bits, and animal B compressed the same information into 10 bits. Both animals have memorized the same information.</p> <p>However, the difference in these memorizations, is that in the future, animal B will likely be able to generalize to more situations. Why, you ask? Intuitively, animal B has likely memorized more of the core reasoning/explanation than animal A, and that will likely explain future situations better. Mathematically, there are also some cool proofs for this (<a href="https://www.youtube.com/watch?v=AKMuA_TVz3A">Ilya talk</a>, <a href="https://arxiv.org/pdf/2304.09355">related paper</a>).</p> <p>Therefore, we can see that further compression of the same information leads to better generalization. As discussed, I believe generalization is likely the most challenging aspect of intelligence, and hence compression is likely core to high levels of intelligence.</p> <p>Perhaps the saying “simplicity is key” hits a bit harder after learning this :)</p> <h4 id="generalization-of-modern-models">Generalization of Modern Models</h4> <p>While some recent papers have stated that LLMs can come up with novel ideas and do research at the level of humans (<a href="https://arxiv.org/pdf/2409.04109">paper1</a>, <a href="https://www.arxiv.org/pdf/2408.06292">paper2</a>), there are numerous problems with the methodology of these papers (that I don’t have the time to write about). If these papers really did work well, then people doing frontier research would be using LLM generated ideas, rather than what people are doing now which is having real humans do research. LLMs, being trained to predict the next token over existing text corpuses online, are nowhere near being capable of generating their own creative ideas that will eventually lead to technological invention. (I’m not claiming here that LLMs can’t assist in the idea generation process, as I agree they are helpful with that. Rather, I’m claiming they are nowhere near being able to generate <em>their own</em>, <strong>good</strong> ideas.)</p> <h3 id="summary">Summary</h3> <p>AI in 2024 (mostly dominated by LLMs) is capable of memorization and knowledge to a wide extent. However, modern models fall short of being able to learn efficiently (few-shot), and are nowhere near being able to generalize to the same extent as humans. As most plants and animals are capable of sample efficient learning, I have no doubt that eventually we will have AI that learns as efficiently as plants and animals. To me, the quadrillion dollar problem is–how do humans generalize so well (so much better than even close ancestors such as apes)? And how will we ever train models to be able to generalize to the same extent as humans, such that they can come up with really great ideas that revolutionize society. <a href="https://alexiglad.github.io/blog/2023/biological_intelligence/">I have a blog about this very topic!</a></p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>One may be tempted to say that creativity need not be defined by composing observations/existing knowledge (i.e. that we can generate entirely new ideas). First, modern research points to the fact that humans are likely generating new ideas based off of combinations of past experiences/observations. Second, as an informal experiment, try thinking of something completely new that is not composed of previous things you have seen/heard/etc. If you manage to succeed at this task (which I would be very surprised about), ask ChatGPT to see if it is truly novel or can be decomposed. If that isn’t convincing, email me, and I’ll try and convince you that you are wrong :). <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="AI,"/><category term="evolution"/><category term="intelligence,"/><category term="memorization,"/><category term="few_shot_learning,"/><category term="generalization,"/><category term="creativity"/><summary type="html"><![CDATA[What are the different components of intelligence?]]></summary></entry><entry><title type="html">EBWM: Cognitively Inspired Energy-Based World Models</title><link href="https://alexiglad.github.io/blog/2024/EBWM/" rel="alternate" type="text/html" title="EBWM: Cognitively Inspired Energy-Based World Models"/><published>2024-06-16T00:00:00+00:00</published><updated>2024-06-16T00:00:00+00:00</updated><id>https://alexiglad.github.io/blog/2024/EBWM</id><content type="html" xml:base="https://alexiglad.github.io/blog/2024/EBWM/"><![CDATA[<p><a href="https://arxiv.org/abs/2406.08862">Paper: https://arxiv.org/abs/2406.08862</a></p> <h2 id="tldr">TLDR:</h2> <p>We developed a new approach for Autoregressive (AR)/world modeling<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> called Energy-Based World Models (EBWM).</p> <p><img src="/assets/img/blog/ebwm_comparison.png" alt="Comparison of EBWM to Existing Autoregressive Approaches" width="1000"/></p> <p>This is inspired by several facets of human cognition, such as System 2 thinking from psychology (which is prolonged, deliberate, and dynamic thinking). Existing AR approaches cannot achieve all cognitive facets discussed in the paper while EBWM can.</p> <p><strong>EBWM scaled better than traditional autoregressive transformers</strong> in data efficiency and GPU hours at sequence modeling in CV.</p> <hr/> <p><em>This blog is written for readers not familiar with all topics discussed, and therefore makes simplifications when describing various topics. Feel free to skip certain sections that are review for you.</em></p> <h3 id="problems-with-current-models">Problems with Current Models</h3> <p>Large Language Models (LLMs) have achieved amazing feats, but still struggle to reason and plan. So how can we train LLMs, and more broadly, world models to reason and plan like humans? Our new work, <a href="https://arxiv.org/abs/2406.08862">Cognitively Inspired Energy-Based World Models</a>, aims to address this.</p> <p>To understand why current LLMs/world models cannot reason and plan at the level of humans, consider the two questions below:</p> <p>What is 2 + 2?</p> <p>What is 67 * 54?</p> <p>These questions vary drastically in difficulty—the first you could probably solve without even deliberately thinking but the second would take deliberate mathematical reasoning. Yet, most existing world models are trained to make every prediction with a fixed amount of computational resources (If you’re thinking CoT is a counterexample to this check the paper!) As humans, we naturally adjust the computational resources we use towards the difficulty of the problem, which is more broadly referred to as System 2 thinking in psychology.</p> <p>We trained world models to be able to approximate aspects of System 2 thinking, through developing ‘Energy-Based World Models’ (EBWM). Not only are these models able to dynamically allocate compute based on the difficulty of a given problem during inference, but they can also achieve several other facets of cognition not possible with existing traditional autoregressive approaches (e.g. Autoregressive Transformers, RNNs, Diffusion Transformers, etc).</p> <p>This matters, because it provides promising steps towards training world models that can truly think, reason, and plan, at the same level as humans—a problem I’m confident is one of the largest blockers towards achieving human-level intelligence/AGI.</p> <p>Before we continue, let’s review what some of the topics we’ll be discussing are.</p> <h2 id="review">Review</h2> <h3 id="world-models">World Models</h3> <p>Intuitively, world models are predictive models of the world. They have learned various laws of physics, such as gravity, and know how people move, how the sun sets, and various other aspects of the world. The world models that our paper focused on are a specific instance of world models where there are several sensory inputs (i.e. multiple video frames) and the model is trained to predict the next sensory input (i.e. the next frame)<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Mathematically, this is equivalent to the following:</p> \[x(t+1) = F(x(1), x(2), \ldots, x(t))\] <p>Where $x(1), …, x(t+1)$ are sensory inputs (i.e. different video frames) and F is the function (or more specifically energy function) being learned.</p> <h3 id="energy-based-models">Energy-Based Models</h3> <p>Energy-Based Models (EBMs) assign a scalar value, called the energy, to each configuration of inputs to a model. The key idea is that configurations with lower energy are more probable and compatible with each other, while those with higher energy are less likely. The goal of an EBM is to learn an energy function (which is usually just a neural network/the model) that gives lower energy to “correct” or “desirable” configurations, like real data points, and higher energy to “incorrect” or “undesirable” ones, like noise or outliers.</p> <p>In the context of world models trained as EBMs, you can think of this energy as how probable/compatible the <em>predicted next state</em> is with the <em>context/past</em>.</p> <p>For example, if the given context was a video of a red ball flying up through the air, a <em>high</em> energy continuation may be a video of a dog chewing on its toy, while a <em>low</em> energy continuation might be a red ball flying down through the air. This red ball flying down through the air is more compatible with the context, which implies lower energy.</p> <h3 id="markov-chain-monte-carlo">Markov Chain Monte Carlo</h3> <p>EBMs trained with neural networks (NNs) have some cool characteristics. Since NNs are fully differentiable, these models can use a method called Markov Chain Monte Carlo (MCMC). Simply put, MCMC under the context of EBMs is a technique that helps models start from random noise and iteratively improve upon their predictions. It does this by minimizing the learned energy function, or by backpropagating the gradient from the energy scalar to the input being improved upon. You can think about this intuitively as asking the model, “How can I adjust this input to improve its likeliness”, and the model telling you how to adjust each and every aspect of the input (i.e. each pixel in the video frame) to minimize the energy and therefore increase that input’s compatibility with the other inputs.</p> <h3 id="ebwm">EBWM</h3> <p>Now, let’s take a look at EBWM in NLP and CV (note that we visualize words and images for NLP and CV respectively, but note that this is all done at the token/embedding level):</p> <p><img src="/assets/img/blog/ebwm.png" alt="EBWM in NLP and CV" width="1000"/></p> <p>The orange boxes here represent the context. The yellow boxes represent the predicted future state (which starts as random noise). First, EBWM predicts the energy of the initially predicted future state (which is likely to be high since it is random noise). Then, using MCMC, this prediction is iteratively improved upon. This ability to dynamically improve upon predictions, while giving an energy score for each of them, allows the models to achieve several cognitive feats described below.</p> <h3 id="energy-based-transformer">Energy-Based Transformer</h3> <p>The Energy-Based Transformer can just be thought of as a decoder transformer with causal attention specifically made to be parallelizable for EBMs. To understand why this is necessary, please check the <a href="https://arxiv.org/abs/2406.08862">paper</a>.</p> <h3 id="implementation-details">Implementation Details</h3> <p>The pseudocode is really helpful for understanding how this is implemented. Feel free to skip to the next subsection if you already get it.</p> <p>Below is the pseudocode for a traditional autoregressive decoder only transformer (in CV):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">SmoothL1Loss</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="n">input_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">next_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>

<span class="n">refined_embeddings</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer</span><span class="p">(</span><span class="n">input_embeddings</span><span class="p">)</span>
<span class="n">predicted_embeddings</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">output</span><span class="p">(</span><span class="n">refined_embeddings</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">predicted_embeddings</span><span class="p">,</span> <span class="n">next_embeddings</span><span class="p">)</span>
</code></pre></div></div> <p>Below is the pseudocode for EBWM (in CV):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pseudocode in PyTorch for loss calculation, similar to Wang Et Al. (https://arxiv.org/abs/2302.01384)
</span>
<span class="c1"># make sure to enable gradient tracking, i.e. wrap with torch.set_grad_enabled(True)
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">set_grad_enabled</span><span class="p">(</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">SmoothL1Loss</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">predicted_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
    <span class="c1"># different corruption techniques can be used, described in C.2 of the main paper (https://arxiv.org/abs/2406.08862)
</span>    <span class="n">next_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_mcmc_steps</span><span class="p">):</span>
        <span class="c1"># Detach embeddings so that no gradient flows through past steps
</span>        <span class="n">predicted_embeddings</span> <span class="o">=</span> <span class="n">predicted_embeddings</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>
        
        <span class="c1"># Refine embeddings through the Energy-Based Transformer (EBT)
</span>        <span class="n">refined_embeddings</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer</span><span class="p">(</span><span class="n">predicted_embeddings</span><span class="p">)</span>
        
        <span class="c1"># Predict energies through a linear layer (energy predictor)
</span>        <span class="n">predicted_energies</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">energy_predictor</span><span class="p">(</span><span class="n">refined_embeddings</span><span class="p">)</span>
        
        <span class="c1"># Compute the gradient of predicted energies w.r.t. predicted embeddings
</span>        <span class="n">predicted_embeddings_grad</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">predicted_energies</span><span class="p">.</span><span class="nf">sum</span><span class="p">(),</span> 
                                                        <span class="n">predicted_embeddings</span><span class="p">,</span> 
                                                        <span class="n">create_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Perform gradient descent w.r.t. the energy function where self.alpha is the learnable MCMC 'learning rate'
</span>        <span class="n">predicted_embeddings</span> <span class="o">=</span> <span class="n">predicted_embeddings</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">predicted_embeddings_grad</span>
        
        <span class="c1"># Calculate reconstruction loss based on predicted and ground truth embeddings
</span>        <span class="n">reconstruction_loss</span> <span class="o">+=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">predicted_embeddings</span><span class="p">,</span> <span class="n">next_embeddings</span><span class="p">)</span>


</code></pre></div></div> <p>A similar intuition applies to NLP. The biggest differences are mapping to/from the vocabulary space. In the case of a traditional autoregressive transformer, this means mapping from the embedding space to the vocabulary space after the final transformer block (a linear layer, self.output in the pseudocode). In the case of EBWM, since predicting the next token is done in the input space rather than the output space, a mapping from the vocabulary space to the embedding space must be done before the first transformer block (again with a linear layer).</p> <h3 id="so-what-other-feats-can-ebwm-achieve-that-existing-autoregressive-approaches-cant">So what other feats can EBWM achieve that existing autoregressive approaches can’t?</h3> <p>The paper discusses four capabilities, which are listed below:</p> <ul> <li>Predictions shape internal state</li> <li>Evaluation of predictions</li> <li>Dynamic allocation of resources</li> <li>Modeling uncertainty in continuous state spaces</li> </ul> <p>Traditional autoregressive transformers and RNNs cannot achieve any of these capabilities, while diffusion models can only achieve two. EBWM can achieve all four, and this motivates our pursuance of the EBWM architecture.</p> <h3 id="so-what-are-the-main-technical-contributions">So what are the main technical contributions?</h3> <p>There are four main contributions in the main paper–the first is the proposal of this architecture motivated by facets of human cognition. Hopefully, by now you understand this intuition :).</p> <p>Second, was the design of an <em>Energy-Based Transformer.</em> EBM’s have failed to stay mainstream in the era of modern ML, so by developing a parallelizable transformer architecture specifically for EBM’s we hope to advance the EBM paradigm. This transformer implementation was a core part of EBWM scaling faster than traditional AR transformers.</p> <p>Third, we investigated EBWM scaling in CV and NLP.</p> <p>The fourth, <em>not discussed much in the main paper</em> contribution, was the usage of a regularized (by reconstruction) objective rather than a contrastive objective for pre-training. There are existing papers that approached autoregressive modeling with an EBM, but they all used contrastive objectives. Contrastive objectives suffer from the curse of dimensionality, which makes regularized objectives more attractive for scaling. This is the key reason behind why EBWM actually scales, unlike other EBM approaches.</p> <h3 id="takeaways-from-the-paper">Takeaways from the Paper</h3> <p>The main takeaway I aim to convey is that EBWM offers an exciting opportunity for training models capable of System 2 thinking. The experiments show promising scaling, so I think further scaling (people with more GPUs :) and more investigations of the capablities after further scaling could be very exciting!</p> <p>Thanks to all coauthors for all the help with this work, and I’m super excited to see what this work leads to in the future!</p> <h2 id="references">References</h2> <p>See references in the paper.</p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>I will use the term world modeling in this blog to broadly refer to all autoregressive models/LLMs. See the paper for more details on why I do this. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>A more general formulation of world models is in the paper, where more than just sensory inputs are conditioned on when predicting the next state/input. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="AI,"/><category term="neuroscience"/><category term="deep_learning,"/><category term="world_models,"/><category term="energy-based_models,"/><category term="autoregressive,"/><category term="transformers,"/><category term="cognition,"/><category term="system_2"/><summary type="html"><![CDATA[What are Energy-Based World Models and why should I care about them?]]></summary></entry><entry><title type="html">How Can We Improve Governments?</title><link href="https://alexiglad.github.io/blog/2024/improving_governments/" rel="alternate" type="text/html" title="How Can We Improve Governments?"/><published>2024-01-03T00:00:00+00:00</published><updated>2024-01-03T00:00:00+00:00</updated><id>https://alexiglad.github.io/blog/2024/improving_governments</id><content type="html" xml:base="https://alexiglad.github.io/blog/2024/improving_governments/"><![CDATA[<h4 id="disclaimer">Disclaimer</h4> <p>I am a student studying Computer Science/Machine Learning and have limited expertise in political science/economics/government. The ideas presented in this blog are my personal opinions and speculative thoughts, not rooted in extensive academic research or professional experience in these fields. My intention is to contribute to the ongoing conversation by presenting a unique perspective. Having been taught in American schools my whole life, I also realize that my perspective on things like communism is likely biased. I also hope to not come across as very pro or anti communism as I do not have very strong opinions on the matter. This was just a random idea I thought had merit :).</p> <h4 id="motivation-in-humans">Motivation in Humans</h4> <p>Reward is at the epicenter of our behavior as agents in this closed feedback loop we call our universe. Reward serves as a reinforcement signal in driving behavior, providing further motivation to pursue an objective. It’s from this perspective that we will analyze the flaws of communism/governments and use it to consequently propose a potential solution.</p> <h4 id="communism">Communism</h4> <p>When I first started learning about communism I thought it was a beautiful idea. The concept of a utopian society with no inequality/inequity and where everyone has equal opportunity appealed to my idealistic nature. So what was the catch–why was/is the U.S. government so against such an ideology?</p> <p>Throughout several lectures on other communist governments the teachings of U.S. schools began to answer this question for me with a cold hard reality check. <em>It hasn’t worked well.</em></p> <p>So why has communism failed to live up to its potential?</p> <p>One of the most common issues for the failure of communism is a corrupt government. When officials get in power, rather than acting in the best interest of their citizens, we have often seen that they act in their own self-interest. This can be linked to the ultimate driver of human nature–‘reward maximization’–and people experiencing that acting with personal interests in mind (i.e. money) maximizes reward much more effectively than acting selflessly.</p> <h4 id="a-potential-solution">A Potential Solution</h4> <p>We know that personal gain is among one of the biggest reasons for communist governments not succeeding, so how can we eradicate this motivation to make people act in a selfless manner? </p> <p>The most obvious solution to me is to erase any sort of potential gain from all leadership positions. In practice, this would look like ensuring that the bank accounts, personal assets, and overall wealth of anyone in political power does not exceed that of the average citizen during and after the position of power. By imposing this limitation, the hope is that people in power no longer have motivation to act in their own selfish interest as the best thing that could happen is to earn as much as everyone else. To enforce this, people’s bank accounts, expenditures, transactions, and assets would need to be watched to ensure politicans do not profit.</p> <p>I can also imagine an even more extreme version of this idea where politicans are forced to live humbly in a small cabin in the woods after maintaining a political position. At this point, serving as a politician would become ‘the ultimate sacrifice’–a position that one serves knowing they will not experience personal gain from. Being elected would become a selfless sacrifice to the citizens of a country. This selfless sacrifice, in my opinion, is what serving as a politician should have always been. Nothing more than doing what’s best for a country without ones cares in sight.</p> <p>This idea also has merit in systems like capitalism or socialism in reducing the corruption of high rank political officials. I can imagine a world where the president of the United States has to agree to live humbly in a cabin after presidency in order to ensure he does not personally benefit from the position. This could also of course be done for other high-up officials.</p> <h4 id="drawbacks">Drawbacks</h4> <p>The biggest drawback to this idea that I can think of is the fact that it requires politicians in power to approve of it. What corrupt politician would want to vote for something that limits how they could benefit financially from a political position? Thus, I think reforming an existing government to employ this idea, especially when an existing government has a high rate of corruption, would be very challenging. Perhaps the best way to employ this idea would be to restart with a completely new government in place of an old one <sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.</p> <p>Another drawback is practicality. Implementing and enforcing this system would be very challenging. Many politicians already find ways to bend rules, so enforcing the rules in this system would need to be a top priority. Additionally, it would require many changes to the structures in existing governments.</p> <p>One potential drawback, that I believe is also a potential benefit, is the challenge in attracting qualified candidates. If this system was implemented, and the rules were enforced as stated, who would want to serve in such a sacrificial position? Would this result in many capable candidates <em>not</em> running for political positions?</p> <p>I see this as a pro and a con. On one hand, this may reduce the set of capable candidates pursuing a position, therefore increasing the probability of a poor candidate landing in office. On the other hand, it could result in the filtering down of candidates to only those who are truly selfless. Additionally, the sacrifice associated with becoming a politician could attract more people who are selfless.</p> <p>It’s possible that politicians, knowing they will live humbly after serving in office, would adapt their approach to developing and approving policies. I also see this as a pro and a con, as it has the potential to make politicians care more about the lower class, but could also result in politicians focusing more on short-term objectives.</p> <h4 id="acknowledgements">Acknowledgements</h4> <p>Huge thank you to Sion Kim for helping me come up with this random idea during a lunch! Thank you to my mom for helping me brainstorm as well!</p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Or maybe, if a country’s citizens were really fond of this idea, seeing how politicians respond to the idea could provide insight into which politicians are corrupt :). This could help determine which politicians to elect to promote this idea. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="politics"/><category term="government,"/><category term="communism,"/><category term="capitalism,"/><category term="corruption,"/><category term="motivation,"/><category term="reinforcement_learning"/><summary type="html"><![CDATA[How can we change motivations to improve governments?]]></summary></entry><entry><title type="html">The Neuroplasticity Hypothesis</title><link href="https://alexiglad.github.io/blog/2023/neuroplasticity_hypothesis/" rel="alternate" type="text/html" title="The Neuroplasticity Hypothesis"/><published>2023-11-21T00:00:00+00:00</published><updated>2023-11-21T00:00:00+00:00</updated><id>https://alexiglad.github.io/blog/2023/neuroplasticity_hypothesis</id><content type="html" xml:base="https://alexiglad.github.io/blog/2023/neuroplasticity_hypothesis/"><![CDATA[<p><em>Idea formulated in June 2021 - blog started during date above.</em></p> <ul> <li><a href="#the-neuroplasticity-hypothesis">The Neuroplasticity Hypothesis</a> <ul> <li><a href="#relu-intuition">ReLU Intuition</a></li> <li><a href="#neuroplasticity">Neuroplasticity</a></li> <li><a href="#the-neuroplasticity-hypothesis-1">The Neuroplasticity Hypothesis</a></li> <li><a href="#residual-connections-and-densenet">Residual Connections and Densenet</a></li> </ul> </li> <li><a href="#related-works">Related Works</a></li> <li><a href="#looking-forward">Looking Forward</a></li> <li><a href="#references">References</a></li> <li><a href="#footnotes">Footnotes</a></li> </ul> <h4 id="relu-intuition">ReLU Intuition</h4> <p>When I was first started studying machine learning I was surprised to discover that ReLU, despite its simplicity, worked well across so many architectures. I remember thinking that its asymmetry about the y-axis ran counterintuitive to what I thought a nonlinearity should look like. To this day, ReLU’s success is still not widely understood or agreed upon.</p> <p>There are some solid hypotheses at least partially explaining why ReLU works well–such as high computational efficiency, sparsity, mitigating the vanishing gradient problem, and biological plausibility. However, there are also certain <em>bad</em> characteristics, such as the dying ReLU, whose impact has not seemed to stop ReLU’s success. Today I urge you to ask a question–what if that ReLU dying wasn’t so bad after all? <em>What if your network didn’t need that neuron?</em></p> <h4 id="neuroplasticity">Neuroplasticity</h4> <p>The human brain does an amazing job at adapting to new information, experiences, and environments, a capability known as neuroplasticity [<a href="#1">1</a>]. This remarkable feature allows the brain to reorganize itself by forming new neural connections throughout life, enabling learning, memory development, and recovery from brain injuries. In response to experience, neurons in the brain form stronger connections, or synapses, when they are frequently used, embodying the “use it or lose it” principle. This strengthening, known as synaptic plasticity, is fundamental to learning and memory. Conversely, synapses weaken or are eliminated when they are seldom used, optimizing brain efficiency by removing less useful connections. This dynamic process ensures that our brains are continually shaped and reshaped by our experiences and interactions with the world.</p> <p>So we know that the human brain does a great job at learning and adapting due to neuroplasticity—how does deep learning leverage neuroplasticity?</p> <p>In Artifical Neural Networks (ANNs) the connections between neurons can strengthen or weaken, similar to how synapses strengthen or weaken within the human brain. This is the crux of the “learning” in deep learning. However, there is one aspect of neuroplasticity within the brain that is not captured very well within deep learning, which is structural neuroplasticity. Structural neuroplasticity refers to the brain’s ability to physically change its structure over time. In the framework of deep learning, this is not generally possible due to the inherent layer structure of ANNs. Rather, in the case of <em>vanilla</em> fully connected layers, all neurons in a layer are connected to all neurons in the subsequent layer—but are not directly connected to any layers before or after that layer and cannot create or remove connections.</p> <h4 id="the-neuroplasticity-hypothesis">The Neuroplasticity Hypothesis</h4> <p>The Neuroplasticity Hypothesis posits that the success of several architectural components within deep learning, such as ReLU as well as Residual Connections, can be attributed to their ability to approximate structural neuroplasticity in the brain.</p> <p>As discussed earlier, using ReLU can cause the dying ReLU “issue”. However, in the case of structural neuroplasticity, this “issue” begins to look like an advantage. Particularly, using ReLU allows for the simulation of neurons effectively being removed from a model, simulating useless synapses being removed in the brain. Thus, if a neuron is not necessary for solving a problem, and it constantly receives a negative gradient, it being dead can help ensure a neural network does not use it! With traditional nonlinearities such as sigmoid or tanh this characteristic would be more challenging or impossible to approximate.</p> <h4 id="residual-connections-and-densenet">Residual Connections and Densenet</h4> <p>Residual Connections [<a href="#4">4</a>] <em>somewhat<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></em> simulate neurons being able to connect to other layers. If this connection ends up being useful, it will be optimized for, resulting in neurons that leverage it. However, if hurtful, residual connections will optimize to avoid using the intermediate layer and approximate the identity function. This allows neural network’s behavior to adapt to simulate including one layer rather than two where the residual connection exists. Therefore, both of these characteristics approximate some aspects of structural neuroplasticity, giving another unique perspective on why Residual Connections work!</p> <p>Densenet [<a href="#5">5</a>] can be seen as similar to Residual Connections—just without the ability to approximate the identity function. Particularly, the characteristic feature of Densenets having layers concatenated to every other future layer can be seen as similar to neurons being able to connect to any arbitrary neuron. Therefore, this also simulates some portions of structural neuroplasticity.</p> <h2 id="related-works">Related Works</h2> <p>Neural Architecture Search (NAS) [<a href="#2">2</a>] is perhaps the most related work to this hypothesis within deep learning. So why has NAS been less popular in recent years?</p> <p>Honestly, I don’t have the best response to this as I have not extensively studied NAS and was not in the field when it was popular. However, my primary hypothesis is the nature of most NAS algorithms requiring bilevel optimization or having a discrete set of candidate architectures.</p> <p>Ideally, rather than having an outer loop and trying things, either through gradient descent as in DARTS [<a href="#3">3</a>], or via complex search algorithms over a discrete state space, NAS would work dynamically within a single training loop over a single model. That is, the layers/structures helpful for performance would dynamically be added or removed.</p> <p>This is somewhat simulated in the one-shot model [<a href="#6">6</a>], where a single model is trained that contains all operations. However, this still has a limitation in that it generally drops an entire operation over a layer or a whole layer at once. Ideally, operations over singular neurons would be able to be learned during training as they are during the constant adaptation of the human brain.</p> <h2 id="looking-forward">Looking Forward</h2> <p>The first iductive bias within deep learning that I see being explored due to this hypothesis is the natural layer structure of deep neural networks. Particularly, one could try to train models without layers, but rather, neurons being able to connect to any other neuron. This would ultimately transform artificial neural networks from having a sequential layer based structure to having a graph based structure—better simulating neuroplasticity within the human brain.</p> <p>The biggest issue with taking this step immediately is computational efficiency. Conducting a forward pass given nodes of a graph with modern hardware is much more expensive than conducting a forward pass with sequential layers when the number of graph nodes is significantly higher than the number of layers. Additionally, this would require a differentiable algorithm to dynamically create and remove connections (synapses) between arbitrary neurons.</p> <p>Long-term, I expect architecture/techniques that further simulate structural neuroplasticity within deep learning to become more and more popular for the same reasons as the architectures/components mentioned above. This could look like more learnable parameters as well as removing more inductive biases that limit structural neuroplasticity.</p> <h2 id="references">References</h2> <p><a name="1"></a>[1] Puderbaugh, Matt, and Prabhu D. Emmady. “Neuroplasticity.” In StatPearls [Internet]. StatPearls Publishing, 2023.</p> <p><a name="2"></a>[2] Weng, Lilian. (Aug 2020). Neural architecture search. Lil’Log. https://lilianweng.github.io/posts/2020-08-06-nas/.</p> <p><a name="3"></a>[3] Liu, Hanxiao, Karen Simonyan, and Yiming Yang. “Darts: Differentiable architecture search.” arXiv preprint arXiv:1806.09055 (2018).</p> <p><a name="4"></a>[4] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. “Deep residual learning for image recognition.” In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016.</p> <p><a name="5"></a>[5] Huang, Gao, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. “Densely connected convolutional networks.” In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708. 2017.</p> <p><a name="6"></a>[6] Bender, Gabriel, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. “Understanding and simplifying one-shot architecture search.” In International conference on machine learning, pp. 550-559. PMLR, 2018.</p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>I say Residual Connections <em>somewhat</em> simulate neurons being able to connect to future layers because they can be seen as being connected to a future layer through an identity matrix (resulting in the summation operation). However, in the standard definition of layers being connected involving a learnable parmeter matrix for the weight or <em>synaptic connection</em> between each neuron, they are not connected. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="AI,"/><category term="neuroscience"/><category term="deep_learning,"/><category term="neuroplasticity,"/><category term="ReLU,"/><category term="residual_connections"/><summary type="html"><![CDATA[What do many of the most impactful deep learning architectural components have in common?]]></summary></entry><entry><title type="html">Does Biological Intelligence Have Any Advantages Over Digital Intelligence?</title><link href="https://alexiglad.github.io/blog/2023/biological_intelligence/" rel="alternate" type="text/html" title="Does Biological Intelligence Have Any Advantages Over Digital Intelligence?"/><published>2023-10-04T00:00:00+00:00</published><updated>2023-10-04T00:00:00+00:00</updated><id>https://alexiglad.github.io/blog/2023/biological_intelligence</id><content type="html" xml:base="https://alexiglad.github.io/blog/2023/biological_intelligence/"><![CDATA[<p>I found Geoffrey Hinton’s <a href="https://www.youtube.com/watch?app=desktop&amp;v=rGgGOccMEiY&amp;ab_channel=CSERCambridge">“Two Paths to Intelligence”</a> talk particularly fascinating. In addition to his strong sense of humor, despite discussing the possible end of the world, I was thoroughly impressed by Dr. Hinton’s ability to reason about the differences in biological and digital intelligence.</p> <p>One point stuck out to me the most–particularly his point that digital intelligence can be shared rapidly. Currently, it takes less than 30 seconds to upload a mini (not quite as capable as humans yet) digital intelligence using websites such as Github. Once uploaded, almost anyone on the internet can access, download, and use this digital intelligence. This simplified weight sharing takes advantage of the <a href="https://www.hoppersroppers.org/fundamentals/Hardware/2-ComputersareDeterministic.html">deterministic nature</a> of computers for a given instruction set.</p> <p>In contrast, humans cannot currently upload our knowledge to a server or share our ‘weights’. As such, the transfer of biological intelligence occurs through the slow process of communication-based knowledge distillation, where around 39-60 bits of information can be transferred per second [<a href="#1">1</a>][<a href="#2">2</a>]. This rate of information transfer in biological intelligence is significantly less than the rate of information transfer in digital intelligence–where billions or more bits per second can be shared due to the deterministic nature of computers operating on specific instruction sets. This constraint poses a huge restraint on the spread of human intelligence. <em>Imagine if as children information from the encyclopedia and all scientific papers was immediately uploaded to our brains.</em></p> <p>Despite the information transfer bottleneck imposed by knowledge distillation, I believe there is a key benefit of this slow knowledge transfer that I will argue has helped make humans successful as a species: creativity.</p> <p>The gradual accumulation of knowledge in humans begins in early childhood. From the moment of birth, our internal models of the world start to take shape. As we reach developmental milestones, where our understanding of the world has achieved a general comprehension level, a more structured form of learning is introduced. Paricularly, in educational settings, a curriculum incrementally exposes students to various subjects, methodically building upon prior knowledge.</p> <p>This slow and structured learning approach in humans stands in stark contrast to the learning mechanisms in digital intelligence–where AI systems often undergo a different training process. Except in cases of <a href="https://ai.stackexchange.com/questions/40241/what-is-curriculum-learning-in-reinforcement-learning#:~:text=Curriculum%20learning%20is%20a%20training,of%20tasks%20or%20training%20samples.">Curriculum Learning</a>, where AI is taught using a gradual increase in the complexity of training samples to mimic human learning, these systems are usually exposed to an entire training dataset at once with varying levels of difficulty. This method allows them to absorb large amounts of information non-sequentially, contrasting with the human method of progressively building knowledge upon itself.</p> <p>Although these characteristics of digital intelligence may sound beneficial, I believe the slow and organic development of knowledge in biological intelligence has distinct advantages. It allows for diverse interpretations and intuitions to flourish–laying the groundwork for creativity. In biological intelligence, each individual develops their understanding and internal representations of information over time through personal experiences and learning. This process results in unique interpretations and perspectives, essential for the development of new ideas.</p> <p>For example, in learning a specific subject, people’s grasp and internalization of concepts can vary significantly. Some may quickly and intuitively understand the material, while others might need more time and structured guidance to achieve the same level of comprehension. This variance can be attributed to different internal representations of prior knowledge. While this diversity in understanding and representation may present challenges in teaching, I believe that it is precisely this diversity in addition to the slow building of an intuition through curriculum learning that breeds creativity. Particularly, this diversity results in a rich landscape of perspectives, where each person’s intuition, shaped by their unique experiences and knowledge, guides them towards exploring different ideas and solutions–eventually leading to what we call innovation.</p> <p>If we accept this viewpoint that diversity in beliefs and representation as well as slow curriculum learning are all potentially responsible for human creativity, some questions arise:</p> <ul> <li>Is human hardware being non-uniform, or its potential non-determinism<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, an advantage in innovative capabilities?</li> <li>Does the diversity and complexity of biological neural hardware in humans play a crucial role in fostering creativity, suggesting that modern computers, with their current deterministic hardware architectures, may inherently lack the capability to exhibit creativity in the same manner as humans?</li> <li>How can we increase the creative capabilities of digital intelligences? Is curriculum learning, being similar to how humans learn, a possible solution? Is reinforcement learning a solution to increasing creativity, as stated by people such as <a href="https://www.youtube.com/watch?v=OPZxs6IXH00&amp;t=850&amp;ab_channel=AlignmentWorkshop">Ilya Sutskever</a> and <a href="https://www.youtube.com/watch?v=17NrtKHdPDw&amp;list=WL&amp;index=2&amp;t=42s&amp;ab_channel=RAIL">Sergey Levine</a>.</li> </ul> <p>I hope this blog sparked some thoughts regarding what differences currently exist between biological and digital intelligence–and how this impacts characteristics of intelligence such as creativity! It’s also important to note that the definition of digital intelligence and associated training approaches will most definitely change over the coming years/decades!</p> <h2 id="updates">Updates</h2> <p>This idea is further supported by a work comparing the capabilities of AI and children with regards to innovation [<a href="#3">3</a>]. This work found that children were able to find novel causal relationships, or innovate, better than the best of today’s Large Language Models (LLMs). In contrast, they found that modern LLMs excel at tasks involving imitation–relying on their large pretraining corpus. Therefore, this work further supports the idea that modern digital intelligences do not have the same creative abilities that humans do.</p> <h2 id="references">References</h2> <p><a name="1"></a>[1] Christophe Coupé et al. , “Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche.” Sci. Adv. 5, eaaw2594 (2019). DOI: 10.1126/sciadv.aaw2594</p> <p><a name="2"></a>[2] Reed, Charlotte M., and Nathaniel I. Durlach. “Note on information transfer rates in human communication.” Presence 7, no. 5 (1998): 509-518.</p> <p><a name="3"></a>[3] Yiu, Eunice, Eliza Kosoy, and Alison Gopnik. “Imitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?.” arXiv preprint arXiv:2305.07666 (2023).</p> <p><em>This blog was developed with the assistance of ChatGPT.</em></p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>I believe that humans do not impose randomness onto the universe and therefore are completely deterministic (<a href="https://en.wikipedia.org/wiki/Hard_determinism">hard-determinism</a>). However, the question of humans imposing randomness versus determinism remains a subject of ongoing philosophical debate. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="AI"/><category term="intelligence,"/><category term="creativity,"/><category term="diversity,"/><category term="curriculum_learning"/><summary type="html"><![CDATA[How does creativity come about?]]></summary></entry></feed>