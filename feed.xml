<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://alexiglad.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://alexiglad.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-15T19:49:20+00:00</updated><id>https://alexiglad.github.io/feed.xml</id><title type="html">blank</title><subtitle>Alexi Gladstone&apos;s official website. </subtitle><entry><title type="html">The Future of Multimodal Does not Involve Text</title><link href="https://alexiglad.github.io/blog/2025/future_of_multimodal/" rel="alternate" type="text/html" title="The Future of Multimodal Does not Involve Text"/><published>2025-12-29T00:00:00+00:00</published><updated>2025-12-29T00:00:00+00:00</updated><id>https://alexiglad.github.io/blog/2025/future_of_multimodal</id><content type="html" xml:base="https://alexiglad.github.io/blog/2025/future_of_multimodal/"><![CDATA[<p><strong>Higher-order thoughts:</strong> <em>This is a belief I‚Äôve had for ~2 years now, that I got after working primarily in multimodal learning for ~1 year. My belief has only strengthened with time :).</em></p> <p>There‚Äôs a concept in multimodal learning called <em>cross-modal transfer</em>‚Äîthe idea that models trained on multiple modalities (like text and images) could gain <em>shared benefits</em> from learning across them. Basically, learnings from one modality (say, images) would help the model better understand another modality (say, text). This has been a desired result for a while now, and recently it‚Äôs become something of an obsession at frontier labs‚Äîboth for those working on large multimodal models and those chasing <a href="https://arxiv.org/pdf/2405.07987">The Platonic Representation Hypothesis</a>.</p> <p>This characteristic has been sought after for good reason. Achieving strong cross-modal transfer at scale would make models so much better at understanding ‚Ä¶ well everything. Us humans do this effortlessly‚Äîa child who learns the word ‚Äúdog‚Äù from a picture book can instantly recognize a real dog they‚Äôve never seen before, and vice versa.</p> <p>One great example of where cross-modal transfer would <em>really</em> benefit models is LLMs, where text-only LLMs are pretty <a href="https://arxiv.org/pdf/2310.07018">terrible at reasoning about the physical world</a>. Ideally, adding the capacity for LLMs to see via images would make (V)LLMs both better at seeing images (of course), but also <em>better at reasoning solely through text.</em> After all, we‚Äôd be giving models more data, and us humans benefit massively from our visual system in learning how to reason.</p> <p>Unfortunately, this phenomenon has not been observed at all. In fact, often the opposite is observed, where adding vision to base LLMs <a href="https://arxiv.org/abs/2412.03467">makes</a> <a href="https://arxiv.org/abs/2309.10313">their</a> <a href="https://arxiv.org/abs/2402.10884">text</a> <a href="https://arxiv.org/abs/2505.19616">reasoning worse</a> :O. While this can partially be attributed to catastrophic forgetting, where text-based LLMs forget some text knowledge due to training on image-text data, this result is completely sad.</p> <p><em>So why don‚Äôt current models succeed at cross-modal transfer?</em></p> <p>I‚Äôd argue the reason isn‚Äôt tied to model architecture or scale‚Äîit‚Äôs <strong>data alignment</strong>, or how well different modalities match up in their representation, structure, and content.</p> <p>To understand why, consider what we‚Äôre actually asking models to do when we train them on text and images together. Text is <em>discrete</em> and operates at a high level of abstraction‚Äîit‚Äôs pure semantics. An image, on the other hand, is <em>continuous</em> and incredibly low-level‚Äîraw pixel values encoding edges, textures, colors. These two modalities are fundamentally different in structure.</p> <p>Now, you might say ‚Äúbut we have tons of image-text pairs on the internet!‚Äù And that‚Äôs true‚Äîwe do have <em>some</em> alignment between text and images thanks to the labeling humans have done (alt-text, captions, etc.) and the work of data labeling companies. But this alignment exists on a spectrum, and text-image pairs sit pretty low on it. The core issue is that captions are <em>very lossy</em>‚Äîthey compress images down to a handful of semantic concepts while discarding almost everything else. The text caption ‚Äúa dog playing in a park‚Äù throws away the dog‚Äôs breed, pose, the lighting, the texture of the grass, the spatial layout‚Äîbasically all the rich, continuous information in the actual image. We‚Äôre forcing models to learn a mapping between two wildly different representations.</p> <p>To really drive this home, consider the image below:</p> <p><img src="/assets/img/blog/forest_lake.jpg" alt="water and boat with landscape"/></p> <p>How would you describe this <em>exactly</em> in text? Let me try: ‚ÄúAn aerial view of a deep teal-green water cutting between two steep mountainsides covered in dense evergreen forest. A small white boat near the center-right is moving away from the camera, leaving a long V-shaped wake that spreads across the water‚Äôs surface, creating rippled reflections of sunlight on the left side. The left cliff face is partially exposed gray rock with vegetation clinging to ledges. In the background, a thin waterfall cascades down the right mountainside. The water transitions from darker blue-green in the foreground to lighter teal where the sunlight hits the wake.‚Äù</p> <p>That‚Äôs 90+ words, and I <em>still</em> haven‚Äôt captured the exact shade of green on each tree, the precise pattern of the ripples, the shape of every rock formation, the subtle gradients in the water, and a ton of other features. A single large image encodes millions of pixels of continuous information‚Äîit takes a massive amount of text to come close to even approximate just the content, let alone the spatial/temporal structure.</p> <p>So if text and images are so poorly aligned, what modalities <em>are</em> well aligned?</p> <p>Fortunately, nature has already given us a pair of modalities that are <em>beautifully</em> aligned: <strong>video and audio</strong>. Both are continuous signals. Both are high-dimensional and contain low-level information. They share the same temporal structure‚Äîwhen a door slams in a video, you hear it at exactly the same moment. The alignment isn‚Äôt something humans had to create through labeling; it‚Äôs just there.</p> <p>What makes this even better is that video-audio data exists <em>for free</em> in the real world. No expensive labeling or data curation<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. You want aligned multimodal data? Just go outside and hit record, or strap a camera on a baby (I‚Äôm slightly joking with this, but not really, see <a href="https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00039/97495/SAYCam-A-Large-Longitudinal-Audiovisual-Dataset">SAYCam</a>).</p> <p>A great example of people exploiting this inherent alignment is <a href="https://deepmind.google/models/veo/">Veo 3</a>. Unlike previous video generation models that output silent videos (looking at you, Sora), Veo 3 <em>natively</em> generates synchronized audio‚Äîdialogue, sound effects, ambient noise‚Äîall from a single model. The lip sync is surprisingly good, and sounds actually match what‚Äôs happening on screen. This isn‚Äôt post-hoc audio slapped onto generated video; the model is learning from the natural alignment of video and audio in its training data. I‚Äôd bet this shared multimodality is a big part of why Veo 3 works so well, and scaling this up with better architectures will only widen the gap between video-only and video-audio models.</p> <p>If the hypothesis that data alignment is the true bottleneck for cross-modal transfer is correct, the implications for text aren‚Äôt great. Text simply doesn‚Äôt occur at scale in natural alignment with other modalities. Sure, we have audiobooks and podcasts with transcripts, but that‚Äôs a tiny fraction of the video-audio data out there. Even when text <em>is</em> aligned with other modalities, the structural mismatch remains‚Äîyou‚Äôre still mapping between discrete, abstract symbols and high-dimensional very noisy continuous signals that contain much more information.</p> <p>Therefore, my prediction is that in the long-term future of multimodal AI (maybe the next 5-10 years), text takes a back seat. The most powerful multimodal systems will be built on video and audio, with text serving as an interface for human convenience rather than a core modality for learning. These changes would enable us to train large models at scale with data that is gathered completely unsupervised‚Äìjust audio and video from the real world.</p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Of course, data curation pipelines aren‚Äôt strictly necessary for video-audio data, though people still often use them to improve quality.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="AI"/><category term="multimodal,"/><category term="llms,"/><category term="vllms,"/><category term="video,"/><category term="audio,"/><category term="text"/><summary type="html"><![CDATA[What multimodal paradigm will win out?]]></summary></entry><entry><title type="html">Energy-Based Transformers are Scalable Learners and Thinkers</title><link href="https://alexiglad.github.io/blog/2025/ebt/" rel="alternate" type="text/html" title="Energy-Based Transformers are Scalable Learners and Thinkers"/><published>2025-06-01T00:00:00+00:00</published><updated>2025-06-01T00:00:00+00:00</updated><id>https://alexiglad.github.io/blog/2025/ebt</id><content type="html" xml:base="https://alexiglad.github.io/blog/2025/ebt/"><![CDATA[<p><a href="https://energy-based-transformers.github.io/static/pdfs/paper.pdf">Paper: https://energy-based-transformers.github.io/static/pdfs/paper.pdf</a><sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> <a href="https://energy-based-transformers.github.io/">Website: https://energy-based-transformers.github.io/</a></p> <p><strong>TLDR</strong>: We <strong>outscale</strong> (feed-forward) transformers while <strong>generalizing</strong> reasoning/system 2 thinking to any modality/problem <strong>without</strong> requiring verifiable rewardsüòÆ. Energy-Based Transformers are the <strong>first approach</strong> to outscale feed-forward transformers across modalities and with respect to several axes including data, depth, parameters, FLOPs, etc. Energy-Based Transformers can think over every single prediction (i.e. every token in language modeling) and generalize better than existing models.</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/proposed_model.png" alt="EBT Autoregressive Modeling Architecture for text and video" width="850"/> <figcaption>Figure 1: <b>EBT Autoregressive Modeling Architecture for text and video</b></figcaption> </figure> <h2 id="how-can-we-generalize-reasoningsystem-2-thinking">How can we Generalize Reasoning/System 2 Thinking?</h2> <p>Current approaches for reasoning/inference-time compute/System 2 Thinking (I‚Äôll use the term System 2 from now on out just to simplify, the paper contains a strong section on why I believe System 2 is a better term than inference-time compute) in AI generally rely on <strong>verifiable rewards</strong>, or rewards that cannot be hacked under any circumstance and that can be easily evaluated. An example of a verifiable reward is a solution to a math problem, where we know the answer we want the model to give (i.e., we know the answer to 5 + 5 = 10), and therefore we can check the output of the model being equal to 10.</p> <p><em>Seems cool, right? So what are the challenges with this approach?</em></p> <p>Well, first off, the approach relies on the problem being easily verifiable (checking the answer is correct easily). Many problems do not take this form, such as creative writing, which is an inherently subjective domain and therefore not easily verifiable. As humans, we can easily think over a wide variety of tasks such as creative writing, relationships, career choices, and coming up with new ideas. Second, existing approaches only really scale well at thoughts performed over text, <em>we want thinking over any modality!<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></em> Lastly, and most importantly, existing approaches rely on human supervision to give rewards. Us humans (and our close animal relatives), were able to learn how to think and reason without any supervision‚Äîso a truly intelligent AI should be able to do the same!</p> <p>So, in an effort to generalize the System 2 capablities of current models, we ask the most important research question of the paper, which is: <strong>‚ÄúCan we rely entirely on unsupervised learning to develop System 2 Thinking?‚Äù</strong> Relying on unsupervised learning to develop System 2 Thinking would enable models to think on any problem/modality, without relying on any human supervision, just like us humans do!<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p> <p>So this is the capability we sought after in the paper. But, before we can achieve System 2, we first need to know what capabilities (referred to as Facets of cognition) are necessary for reasoning/System 2. In the paper we identify three capabilities inspired by human cognition, which are:</p> <ol> <li>The ability to think for longer (dynamic computation allocation)</li> <li>The ability to express uncertainty (uncertainty in continuous state spaces)</li> <li>The ability to verify whether predictions are correct or not (prediction verification)</li> </ol> <p>While this list may not be completely comprehensive in the quest for human like thinking‚Äîthese capabilities form the basis for System 2. As an intuitive example, if I asked you a question like: <em>‚Äúwhat‚Äôs 57 * 63‚Äù</em>, you‚Äôd probably first realize that you don‚Äôt immediately know the answer (uncertainty and verification), think for a longer amount of time (dynamic computation), and then possibly check (verify) your work! Alternatively, if I asked you <em>‚Äúwhat‚Äôs 2 + 2‚Äù</em> you‚Äôd probably (hopefully :) immediately know the answer is 4, and be able to verify that answer.</p> <p>We can broadly classify existing paradigms (for now we can focus on the autoregressive case) based on how they predict and whether they have these capabilities.</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/model_comparison.png" alt="Comparison of autoregressive EBTs to existing autoregressive approaches" width="800"/> <figcaption>Figure 2: <b>Comparison of autoregressive EBTs to existing autoregressive approaches</b></figcaption> </figure> <figure class="text-center"> <img src="/assets/img/blog/ebt/architectures_cognitive_facets.png" alt="Existing autoregressive architectures and which facets of cognition they have." width="700"/> <figcaption>Figure 3: <b>Existing autoregressive architectures and which facets of cognition they have.</b></figcaption> </figure> <p>Ok, now that we know what the necessary prerequisites for System 2, the question becomes, <em>‚Äúhow can we learn these capabilities from unsupervised learning?‚Äù</em> Well, it turns out that there is a very simple and elegant solution that achieves all three of these capabilities at the exact same time. The idea is to first learn a verifier (a model that tells you the goodness/compatibility of a prediction given some context), and then optimize predictions with respect to this verifier. Learning a verifier immediately solves the problems of expressing uncertainty and verifying predictions, and optimizing predictions with respect to this verifier enables dynamic computation through performing optimization for longer.</p> <p>It turns out that this intuitive idea is actually the definition of Energy-Based Models (EBMs)! EBMs learn to assign a scalar <strong>energy</strong> value (the verification) denoting the goodness/compatibility/unnormalized probability of a set of variables‚Äîwhich in this case is a context and prediction pair. </p> <p>The key idea behind EBMs is that configurations with lower energy are more probable and compatible with each other, while configurations with higher energy are less probable. More particularly, the goal of an EBM is to learn an energy function (which maps inputs to a scalar energy; in the case of our paper the energy function is just the entire neural network) that gives lower energy to ‚Äúcorrect‚Äù or ‚Äúdesirable‚Äù configurations, like real data points, and higher energy to ‚Äúincorrect‚Äù or ‚Äúundesirable‚Äù ones, like noise or outliers.</p> <p>For example, if the given context was a video of a dog running to catch a frisbee, a <em>high</em> energy continuation may be a video of a dog chewing on its toy, while a <em>low</em> energy continuation might be the dog catching the frisee. This dog catching the frisbee scenario is more compatible with the context, which implies lower energy.</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/energy_landscape_minimization.png" alt="EBT Autoregressive Language Modeling Thinking Example" width="600"/> <figcaption>Figure 4: <b>EBT Autoregressive Language Modeling Thinking Example as Energy Minimization</b></figcaption> </figure> <p>‚ÄúThinking‚Äù in these EBMs can be performed by starting with an initial (random) prediction, and then optimizing this prediction by minimizing its energy through gradient descent (shown above).</p> <p>To enable high scalability, we design a specific type of EBMs combined with the Transformer architecture and with a scalable training algorithm, which we call Energy-Based Transformers (EBTs); EBTs enable high training efficiency, stability, and parallelizability. We discover several tricks for training EBTs at scale as well as for enabling System 2 to emerge during pretraining. For more information and details on how these EBTs are actually trained, please reference the approach section of the paper.</p> <p>Ok, so hopefully by now the intuition of EBTs make sense‚Äîbut how do they actually work in practice, does the thinking actually help performance? We conducted experiments to test this by comparing EBTs against standard feed-forward Transformers (we use the SOTA recipe from the <a href="https://arxiv.org/pdf/2312.00752">Mamba paper</a> called the Transformer++) on tasks such as language modeling. You can see from the left subfigure that thinking with EBTs significantly improves performance over feed-forward Transformers. Particularly, by thinking for longer and also self-verifying EBTs can out-generalize feed-forward transformers to Out-of Distribution (OOD) data. It‚Äôs important to note that EBTs here are improving the performance of <strong>every single next token</strong>, and <strong>not just a final reasoning accuracy</strong> like current ‚Äúreasoning‚Äù foundation models. The right subfigure also demonstrates promising results, where the performance from thinking <em>improves with scale</em>, suggesting that EBTs trained at scale will benefit even more from thinking than EBTs trained at the current smaller scale.</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/thinking_performance.png" alt="EBT Thinking Performance" width="800"/> <figcaption>Figure 5: <b>EBT Thinking Performance Compared to Transformer++ and as scale increases</b></figcaption> </figure> <p>Another amazing result (that needs to be tested more) is the effect of System 2 Thinking on generalization to data that varies in Out-of-Distribution (OOD) magnitude (how far away data is from the training distribution). For example, below we can see a plot demonstrating that as data becomes more OOD, the performance gains from thinking increase. This aligns with results in psychology, where System 2 in humans is used to generalize to new unseen scenarios.</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/scaling_thinking_nlp_ar_ood.svg" alt="EBT Thinking Effect on Generalization to OOD Data" width="400"/> <figcaption>Figure 6: <b>EBT Thinking Effect on Generalization to OOD Data</b></figcaption> </figure> <p><em>Ok so we have Generalized System 2 Thinking, but‚Ä¶</em></p> <h2 id="how-come-this-outscales-feed-forward-transformers">How Come this Outscales (Feed-Forward) Transformers?</h2> <p>This is a good question that does not have as definitive of an answer as generalizing reasoning, however, I can give a general intuition backed by two main reasons for why I believe this is occurring:</p> <ol> <li>Learning to verify is (generally) easier than learning to generate.<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> EBTs learn to verify (so that they can generate), whereas feed-forward models just learn to directly generate. Therefore, EBTs generalize better, and this improved generalization leads to improved scaling.</li> <li>EBTs make weaker assumptions about the data prediction process than feed-forward Transformers, while enabling higher model flexibility (predicting data by optimizing w.r.t a verifier, which can involve many forward passes, vs. feed-forward transformers which need to predict data within a single forward pass). Generally, in AI, systems that increase flexibility and decrease assumptions win out over time (there is a <a href="https://www.youtube.com/watch?v=orDKvo8h71o&amp;ab_channel=StanfordOnline">great talk</a> by Hyung Won Chung on this). Thus, it makes sense under this perspective that EBTs scale better.</li> </ol> <p>We conducted several scaling experiments to be as thorough as possible in determining how EBTs scale compared to feed-forward transformers. For example, in all of the experiments shown below for language modeling, we determine the scaling rate of EBTs compared to the Transformer++ by changing just a single independent variable (as is commonly done in science, but not in empirical ‚Äúscaling law‚Äù papers<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>).</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/scaling_nlp_1.png" alt="Scaling trends for EBTs vs Transformer++ (feed-forward Transformers) in Language Modeling 1" width="900"/> <figcaption>Figure 7: <b>Scaling trends for EBTs vs Transformer++ (feed-forward Transformers) in Language Modeling 1</b></figcaption> </figure> <p>Remarkably, the plots demonstrate that EBTs scale up to 35% faster than feed-forward Transformers for data!! This is perhaps the most impressive result of the paper as it suggests that EBTs are 35% more data-efficient than Transformers. This essentially means that at scale, if you needed 30T tokens for a feed-forward Transformer, you‚Äôd need less than 20T for an EBT to achieve the same pretraining perplexity. Almost as impressive is that on downstream tasks, with the same pretraining perplexity, EBTs outperform the Transformer++, suggesting better generalization (these results are in the paper). Together, these results suggest that you can get significantly better downstream task performance while using less data with EBTs compared to the standard Transformer++. The results in the other two plots also demonstrate a similar out-scaling trend for EBTs compared to the Transformer++ when it comes to batch size as well as depth.</p> <p>In fact, if we zoom in a little bit into a similar plot from a scaled up experiment we see that the gap in performance between EBTs and the Transformer++ is <em>actually increasing over time!</em> (Note that this line was fit with a log function).</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/scaling_learning_nlp_ar_data_smallm2.svg" alt="Scaling for EBTs vs Transformer++ (feed-forward Transformers) in Data Scaled up and Zoomed In" width="450"/> <figcaption>Figure 8: <b>Scaling for EBTs vs Transformer++ (feed-forward Transformers) in Data Scaled up and Zoomed In</b></figcaption> </figure> <p>We see similar (although less dramatic) outscaling of EBTs compared to the Transformer++ for parameter/FLOP efficiency (at the scale we tested at, EBTs still lag behind in raw y axis performance, but scale at a higher rate, and therefore would perform better than the Transformer++ asymptotically if these trends continue).</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/scaling_nlp_2.png" alt="Scaling trends for EBTs vs Transformer++ in Language Modeling 2" width="900"/> <figcaption>Figure 9: <b>Scaling trends for EBTs vs Transformer++ in Language Modeling 2</b></figcaption> </figure> <p>In CV, we observe that EBTs very dramatically outscale the Transformer++ at predicting the next frame, achieving a 33% and 34% higher scaling rate for width (embedding dimension) and parameters respectively. (These trends are less consistent than the scaling trends in language modeling though.)</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/scaling_video_1.png" alt="Scaling trends for EBTs vs Transformer++ in Video Modeling" width="600"/> <figcaption>Figure 10: <b>Scaling trends for EBTs vs Transformer++ in Video Modeling</b></figcaption> </figure> <p>We also compared EBTs to DiTs in simple image denoising tasks and achieved very promising results (better quality with less forward passes). For more information on how EBTs work and any details including pseudocode please feel free to reference the <a href="https://energy-based-transformers.github.io/static/pdfs/paper.pdf">paper</a>!</p> <h2 id="conclusion-and-a-sprinkle-of-intuition">Conclusion and a Sprinkle of Intuition</h2> <p>While the results are promising, there is a long way to go in scaling these models up (I‚Äôm mainly looking at you, potential stability issues). But, I‚Äôm confident that in the next 3 years EBTs (or some variant) will be pretty common (let‚Äôs check back and see:). The main reason I see EBTs being adopted, at least in the short term, is the improved generalization and data efficiency (in fact, these things go hand in hand as better generalization -&gt; better data learning efficiency). Strong generalization is by far the most important aspect of any given model (as what else really matters besides generalization), and data efficiency has become increasingly important (see <a href="https://www.youtube.com/watch?v=6nJZopACRuQ&amp;ab_channel=OpenAI">this video</a> by the OpenAI pre-training team where they mention that the biggest blocker to AI progress is more data-efficient algorithms)! For these reasons alone I‚Äôm confident there will be high interest in EBTs, in addition to the System 2 capabilities, but we shall see as the world is challenging to predict.</p> <p>Generally, approaches that increase the flexibility of models scale best in the long run (i.e., see CNNs -&gt; ViTs, statistical learning -&gt; NNs, almost all of AI as a field in general). EBTs are just the next example of this, where (if we squint a little bit) EBTs are more flexible than DiTs<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>, which are more flexible than standard feed-forward models such as RNNs and traditional transformers (assuming they only update with new state information, more on this nuance in the paper).</p> <p>Thanks to all coauthors for all the help with this work, and I‚Äôm super excited to see what this work leads to in the future! Feel free to check the <a href="https://energy-based-transformers.github.io/static/pdfs/paper.pdf">paper</a> for more information and details/references.</p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>The old version of this paper was called ‚ÄúCognitively-Inspired Energy Based World Models or EBWM‚Äù but because of me starting my PhD, working with other people, along with some other things, we thought a rebrand was fitting. We also conducted much more thorough experiments due to having additional compute.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>While there are approaches for multimodal reasoning, these generally still have models think by outputting text. Thinking over continuous signals using RL at scale has not yet succeeded to my knowledge.¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>The paper discusses other forms of System 2 in more depth, such as diffusion/RNNs.¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4"> <p>The intuition section of the paper does a good job at explaining why this is the case, making connections to theoretical computer science. But, just for flavor, consider the case of a maze. What‚Äôs more likely to generalize‚Äîthe maze generator (which has to generate a solution in a single forward pass) or the maze verifier (which only has to verify the correctness of a solution in a single forward pass)?¬†<a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:5"> <p>We also do ‚Äúscaling law‚Äù runs where we vary several independent variables at once following common practice in other ML papers. However, I‚Äôd argue these experiments are much less informative than experiments where just a single independent variable is changed at a time, as these scaling law experiments generally involve changing several parameters at once (data, batch size, depth, width, etc) meaning it‚Äôs not possible to isolate which axes two different models scale better/worse compared to one another. Changing a single indenpendent variable (one axis) at a time allows us to directly measure these things‚Äîthis follows standard scientific methodology :).¬†<a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:6"> <p>We reference the reader to the <a href="https://energy-based-transformers.github.io/static/pdfs/paper.pdf">paper</a> section comparing diffusion and EBMs in depth for why EBMs are more flexible. The TLDR is EBMs are a generalization of diffusion and allow for estimating (unnormalized) likelihoods or verifying at every step of the thinking process, whereas diffusion models only do this implicitly after the entire denoising process.¬†<a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="AI,"/><category term="neuroscience"/><category term="deep"/><category term="learning,"/><category term="ai,"/><category term="reasoning,"/><category term="system-2-thinking,"/><category term="scaling,"/><category term="energy-based-models,"/><category term="energy_based_models,"/><category term="transformers,"/><category term="ebms,"/><category term="verification,"/><category term="scaling-law,"/><category term="test-time-compute,"/><category term="inference-time-compute,"/><category term="cognitively-inspired-energy-based-world-models"/><summary type="html"><![CDATA[What are Energy-Based Transformers and why should I care about them?]]></summary></entry><entry><title type="html">The Different Components of Intelligence</title><link href="https://alexiglad.github.io/blog/2024/intelligence_components/" rel="alternate" type="text/html" title="The Different Components of Intelligence"/><published>2024-09-14T00:00:00+00:00</published><updated>2024-09-14T00:00:00+00:00</updated><id>https://alexiglad.github.io/blog/2024/intelligence_components</id><content type="html" xml:base="https://alexiglad.github.io/blog/2024/intelligence_components/"><![CDATA[<p><em>It‚Äôs worth noting that most of this blog is not very scientific and is mostly just a useful abstraction for how to view the different components of intelligence (which is especially useful for thinking about current model capablities or benchmarking). The different aspects of intelligence are abstract ideas, don‚Äôt necesssarily represent the underlying way the mind works, and may not encompass all different portions of intelligence. If you come up with other aspects of intelligence please email me :)</em></p> <h3 id="1-memorization-and-knowledge">1) Memorization and Knowledge</h3> <p>Memorization broadly refers to the ability to recite information. Knowledge goes a step beyond this, and involves applying memorized information to new contexts. Knowledge often involves understanding at a deeper level than sole memorization.</p> <p>As of 2024, Large Language Models (LLMs) are pretty good at memorization and, are half decent at knowledge, but fail at most of the other aspects of intelligence discussed below.</p> <h3 id="2-learning-efficiency">2) Learning Efficiency</h3> <p>Because of evolution, it‚Äôs easy to see that most modern animals (<a href="https://pubmed.ncbi.nlm.nih.gov/24390479/">and even plants</a>), learn pretty efficiently.</p> <p><a id="image-experiment"></a> </p> <p><img src="/assets/img/blog/dog_experiment.png" alt="object choice task experiment"/></p> <p>As an example of animal learning efficiency, if you put a treat on the floor under a cup, and then put down another cup (with the dog watching), and then point to the cup without a treat under it (<a href="#image-experiment">see example image</a>, <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Flink.springer.com%2F10.1007%2F978-3-319-47829-6_100-1&amp;psig=AOvVaw2cVxlE7-ZVPwfrL25o3WDF&amp;ust=1726449432306000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBcQjhxqFwoTCLDCsbzjw4gDFQAAAAAdAAAAABAE">link</a>), <a href="https://www.psychologytoday.com/us/blog/canine-corner/201502/study-dogs-can-identify-liars-and-they-dont-trust-them">dogs may follow your point</a>. However, often after 1 or a few times doing this dogs will often learn <strong>not</strong> to trust your point. This example demonstrates that dogs will learn quickly given just a few or even 1 example/‚Äôshot‚Äô/trial.</p> <p>Within the context of AI, the idea of learning efficiency <a href="https://arxiv.org/abs/2205.06743">has long been a focus</a>. People often claim that modern models are great at zero-shot or few-shot learning‚Ä¶ but is this really the case?</p> <p>I believe that ‚ÄúZero-shot‚Äù learning (and few-shot similarly) can be interpreted in two distinct ways:</p> <ol> <li> <p><strong>Not seeing something at all during training and being able to predict it on the test set</strong>: E.g. if you were doing supervised classification, not seeing a dog at all during training, but knowing what a dog is during testing. This is a harder problem and, in my opinion, is impossible.</p> </li> <li> <p><strong>Not specifically training or learning for a downstream task but still being able to do the task</strong>: This is relatively easier and has been solved in some cases, such as CLIP for image recognition or LLMs being able to extract entities. These models were not trained for the specific task, yet are still able to perform it. It‚Äôs worth noting though, that these models have seen the concepts/tasks they are being tested on throughout pre-training (or some form of pre-training).</p> </li> </ol> <p>The first definition is the literal definition of zero-shot, while the latter definition is the more commonly used and easier version.</p> <p>So How Does Pre-Training Data Affect ‚ÄúZero-Shot‚Äù Performance?</p> <p>The paper ‚Äú<a href="https://arxiv.org/pdf/2404.04125">Do Multimodal Models Really Achieve Zero-Shot Generalization?</a>‚Äù states the following:</p> <blockquote> <p>‚ÄúWe consistently find that, far from exhibiting ‚Äúzero-shot‚Äù generalization, multimodal models require exponentially more data to achieve linear improvements in downstream ‚Äúzero-shot‚Äù performance, following a sample inefficient log-linear scaling trend.‚Äù</p> </blockquote> <p>This finding aligns with the second definition of ‚Äúzero-shot‚Äù, and supports the idea that with modern AI we are still far from true zero or few shot generalization/learning efficiency (the first definition).</p> <p>Recent benchmarks, such as <a href="https://arcprize.org/">ARC-AGI</a>, further support this. ARC-AGI measures the abilities of models to learn a completely new task, given just a few examples and then perform that task. Humans can achieve nearly 100% on this benchmark, but modern LLMs only score around 21%, and top approaches score less than 50% (both of these values are based off of when this blog was written in 2024). It‚Äôs worth noting that problems in the benchmark get progressively (one could even say exponentially) more difficult, so the jump from 21% to 50% is <strong>very significantly</strong> easier than the jump from 50% to 100%.</p> <details> <summary>If you are now tempted to say, "What about LLMs which are great at few shot learning?"</summary> LLMs are not good at few shot learning unless they have already been trained on data similar to whatever task is being performed. For benchmarks such as ARC, which are completely out of distribution for the pre-training data of LLMs, they do terrible at few shot learning. AI researchers have become accustomed to calling models good at few shot learning, even though models have often seen similar examples during pre-training hundreds or even thousands of times during training. [Paper](https://arxiv.org/pdf/2404.04125) </details> <p><br/></p> <h3 id="3-generalization-creativity-and-invention">3) Generalization, Creativity, and Invention</h3> <p>Generalization, in the context of intelligence, can broadly be defined as ‚Äúthe ability to apply learned knowledge or skills to new and varied situations.‚Äù Similarly, creativity can be thought of as ‚Äúthe ability to generate novel and original ideas or solutions by combining existing knowledge or skills in innovative ways<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.‚Äù It‚Äôs easy to see that the further someone is able to generalize, the better their creativity will likely be. It‚Äôs also worth noting that creativity and generalization are spectrums, and not binary skills that intelligent systems either do or don‚Äôt have.</p> <p>One cool thing about creativity, is that <em>great</em> creativity can lead to invention.</p> <p>Now, if we were to look at our society now, and compare it to societies from 1,000 years ago, 10,000 years, etc; the differences we‚Äôd see could be explained by one thing‚Äìinvention. Inventions, one after another, have led to the revolutionization of our society. Without invention, humans would be no different than apes.</p> <h4 id="the-most-challenging-aspect-of-intelligence">The Most Challenging Aspect of Intelligence</h4> <p>I‚Äôve ordered this blog based off of which aspects of intelligence I believe to be easiest (memorization, knowledge) to hardest (generalization). Although I don‚Äôt think I can argue for this ‚Äòordering of intelligence attributes‚Äô scientifically, a simple evolutionary argument can be made. Animals, and even plants as we have seen, are capable of memorization and few shot learning. However, their abilities to invent with the same success as humans is limited. As such, we can say that humans have better generalization/creativity than other animals and plants, and that this is something extremely challenging to develop through intelligence (since only humans have it).</p> <h4 id="compression">Compression</h4> <p>Tons of AI researchers love mentioning how compression is linked/core to intelligence (<a href="https://arxiv.org/pdf/2309.10668">paper</a>, <a href="https://x.com/arankomatsuzaki/status/1780073500536872990">twitter mention</a>). I‚Äôve always agreed with this statement. To me, however, it was never clear <em>why</em> (at least with an elegant explanation). I think the framework discussed in this blog to look at different aspects of intelligence provides a great lens, here‚Äôs how:</p> <p>Compression in an of itself is an aspect of memorization/knowledge. Let‚Äôs say animal A has compressed 1000 bits of information into 100 bits, and animal B compressed the same information into 10 bits. Both animals have memorized the same information.</p> <p>However, the difference in these memorizations, is that in the future, animal B will likely be able to generalize to more situations. Why, you ask? Intuitively, animal B has likely memorized more of the core reasoning/explanation than animal A, and that will likely explain future situations better. Mathematically, there are also some cool proofs for this (<a href="https://www.youtube.com/watch?v=AKMuA_TVz3A">Ilya talk</a>, <a href="https://arxiv.org/pdf/2304.09355">related paper</a>).</p> <p>Therefore, we can see that further compression of the same information leads to better generalization. As discussed, I believe generalization is likely the most challenging aspect of intelligence, and hence compression is likely core to high levels of intelligence.</p> <p>Perhaps the saying ‚Äúsimplicity is key‚Äù hits a bit harder after learning this :)</p> <h4 id="generalization-of-modern-models">Generalization of Modern Models</h4> <p>While some recent papers have stated that LLMs can come up with novel ideas and do research at the level of humans (<a href="https://arxiv.org/pdf/2409.04109">paper1</a>, <a href="https://www.arxiv.org/pdf/2408.06292">paper2</a>), there are numerous problems with the methodology of these papers (that I don‚Äôt have the time to write about). If these papers really did work well, then people doing frontier research would be using LLM generated ideas, rather than what people are doing now which is having real humans do research. LLMs, being trained to predict the next token over existing text corpuses online, are nowhere near being capable of generating their own creative ideas that will eventually lead to technological invention. (I‚Äôm not claiming here that LLMs can‚Äôt assist in the idea generation process, as I agree they are helpful with that. Rather, I‚Äôm claiming they are nowhere near being able to generate <em>their own</em>, <strong>good</strong> ideas.)</p> <h3 id="summary">Summary</h3> <p>AI in 2024 (mostly dominated by LLMs) is capable of memorization and knowledge to a wide extent. However, modern models fall short of being able to learn efficiently (few-shot), and are nowhere near being able to generalize to the same extent as humans. As most plants and animals are capable of sample efficient learning, I have no doubt that eventually we will have AI that learns as efficiently as plants and animals. To me, the quadrillion dollar problem is‚Äìhow do humans generalize so well (so much better than even close ancestors such as apes)? And how will we ever train models to be able to generalize to the same extent as humans, such that they can come up with really great ideas that revolutionize society. <a href="https://alexiglad.github.io/blog/2023/biological_intelligence/">I have a blog about this very topic!</a></p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>One may be tempted to say that creativity need not be defined by composing observations/existing knowledge (i.e. that we can generate entirely new ideas). First, modern research points to the fact that humans are likely generating new ideas based off of combinations of past experiences/observations. Second, as an informal experiment, try thinking of something completely new that is not composed of previous things you have seen/heard/etc. If you manage to succeed at this task (which I would be very surprised about), ask ChatGPT to see if it is truly novel or can be decomposed. If that isn‚Äôt convincing, email me, and I‚Äôll try and convince you that you are wrong :).¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="AI,"/><category term="evolution"/><category term="intelligence,"/><category term="memorization,"/><category term="few_shot_learning,"/><category term="generalization,"/><category term="creativity"/><summary type="html"><![CDATA[What are the different components of intelligence?]]></summary></entry><entry><title type="html">How Can We Improve Governments?</title><link href="https://alexiglad.github.io/blog/2024/improving_governments/" rel="alternate" type="text/html" title="How Can We Improve Governments?"/><published>2024-01-03T00:00:00+00:00</published><updated>2024-01-03T00:00:00+00:00</updated><id>https://alexiglad.github.io/blog/2024/improving_governments</id><content type="html" xml:base="https://alexiglad.github.io/blog/2024/improving_governments/"><![CDATA[<h4 id="disclaimer">Disclaimer</h4> <p>I am a student studying Computer Science/Machine Learning and have limited expertise in political science/economics/government. The ideas presented in this blog are my personal opinions and speculative thoughts, not rooted in extensive academic research or professional experience in these fields. My intention is to contribute to the ongoing conversation by presenting a unique perspective. Having been taught in American schools my whole life, I also realize that my perspective on things like communism is likely biased. I also hope to not come across as very pro or anti communism as I do not have very strong opinions on the matter. This was just a random idea I thought had merit :).</p> <h4 id="motivation-in-humans">Motivation in Humans</h4> <p>Reward is at the epicenter of our behavior as agents in this closed feedback loop we call our universe. Reward serves as a reinforcement signal in driving behavior, providing further motivation to pursue an objective. It‚Äôs from this perspective that we will analyze the flaws of communism/governments and use it to consequently propose a potential solution.</p> <h4 id="communism">Communism</h4> <p>When I first started learning about communism I thought it was a beautiful idea. The concept of a utopian society with no inequality/inequity and where everyone has equal opportunity appealed to my idealistic nature. So what was the catch‚Äìwhy was/is the U.S. government so against such an ideology?</p> <p>Throughout several lectures on other communist governments the teachings of U.S. schools began to answer this question for me with a cold hard reality check. <em>It hasn‚Äôt worked well.</em></p> <p>So why has communism failed to live up to its potential?</p> <p>One of the most common issues for the failure of communism is a corrupt government. When officials get in power, rather than acting in the best interest of their citizens, we have often seen that they act in their own self-interest. This can be linked to the ultimate driver of human nature‚Äì‚Äòreward maximization‚Äô‚Äìand people experiencing that acting with personal interests in mind (i.e. money) maximizes reward much more effectively than acting selflessly.</p> <h4 id="a-potential-solution">A Potential Solution</h4> <p>We know that personal gain is among one of the biggest reasons for communist governments not succeeding, so how can we eradicate this motivation to make people act in a selfless manner? </p> <p>The most obvious solution to me is to erase any sort of potential gain from all leadership positions. In practice, this would look like ensuring that the bank accounts, personal assets, and overall wealth of anyone in political power does not exceed that of the average citizen during and after the position of power. By imposing this limitation, the hope is that people in power no longer have motivation to act in their own selfish interest as the best thing that could happen is to earn as much as everyone else. To enforce this, people‚Äôs bank accounts, expenditures, transactions, and assets would need to be watched to ensure politicans do not profit.</p> <p>I can also imagine an even more extreme version of this idea where politicans are forced to live humbly in a small cabin in the woods after maintaining a political position. At this point, serving as a politician would become ‚Äòthe ultimate sacrifice‚Äô‚Äìa position that one serves knowing they will not experience personal gain from. Being elected would become a selfless sacrifice to the citizens of a country. This selfless sacrifice, in my opinion, is what serving as a politician should have always been. Nothing more than doing what‚Äôs best for a country without ones cares in sight.</p> <p>This idea also has merit in systems like capitalism or socialism in reducing the corruption of high rank political officials. I can imagine a world where the president of the United States has to agree to live humbly in a cabin after presidency in order to ensure he does not personally benefit from the position. This could also of course be done for other high-up officials.</p> <h4 id="drawbacks">Drawbacks</h4> <p>The biggest drawback to this idea that I can think of is the fact that it requires politicians in power to approve of it. What corrupt politician would want to vote for something that limits how they could benefit financially from a political position? Thus, I think reforming an existing government to employ this idea, especially when an existing government has a high rate of corruption, would be very challenging. Perhaps the best way to employ this idea would be to restart with a completely new government in place of an old one <sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.</p> <p>Another drawback is practicality. Implementing and enforcing this system would be very challenging. Many politicians already find ways to bend rules, so enforcing the rules in this system would need to be a top priority. Additionally, it would require many changes to the structures in existing governments.</p> <p>One potential drawback, that I believe is also a potential benefit, is the challenge in attracting qualified candidates. If this system was implemented, and the rules were enforced as stated, who would want to serve in such a sacrificial position? Would this result in many capable candidates <em>not</em> running for political positions?</p> <p>I see this as a pro and a con. On one hand, this may reduce the set of capable candidates pursuing a position, therefore increasing the probability of a poor candidate landing in office. On the other hand, it could result in the filtering down of candidates to only those who are truly selfless. Additionally, the sacrifice associated with becoming a politician could attract more people who are selfless.</p> <p>It‚Äôs possible that politicians, knowing they will live humbly after serving in office, would adapt their approach to developing and approving policies. I also see this as a pro and a con, as it has the potential to make politicians care more about the lower class, but could also result in politicians focusing more on short-term objectives.</p> <h4 id="acknowledgements">Acknowledgements</h4> <p>Huge thank you to Sion Kim for helping me come up with this random idea during a lunch! Thank you to my mom for helping me brainstorm as well!</p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Or maybe, if a country‚Äôs citizens were really fond of this idea, seeing how politicians respond to the idea could provide insight into which politicians are corrupt :). This could help determine which politicians to elect to promote this idea.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="politics"/><category term="government,"/><category term="communism,"/><category term="capitalism,"/><category term="corruption,"/><category term="motivation,"/><category term="reinforcement_learning"/><summary type="html"><![CDATA[How can we change motivations to improve governments?]]></summary></entry><entry><title type="html">The Neuroplasticity Hypothesis</title><link href="https://alexiglad.github.io/blog/2023/neuroplasticity_hypothesis/" rel="alternate" type="text/html" title="The Neuroplasticity Hypothesis"/><published>2023-11-21T00:00:00+00:00</published><updated>2023-11-21T00:00:00+00:00</updated><id>https://alexiglad.github.io/blog/2023/neuroplasticity_hypothesis</id><content type="html" xml:base="https://alexiglad.github.io/blog/2023/neuroplasticity_hypothesis/"><![CDATA[<p><em>Idea formulated in June 2021 - blog started during date above.</em></p> <ul> <li><a href="#the-neuroplasticity-hypothesis">The Neuroplasticity Hypothesis</a> <ul> <li><a href="#relu-intuition">ReLU Intuition</a></li> <li><a href="#neuroplasticity">Neuroplasticity</a></li> <li><a href="#the-neuroplasticity-hypothesis-1">The Neuroplasticity Hypothesis</a></li> <li><a href="#residual-connections-and-densenet">Residual Connections and Densenet</a></li> </ul> </li> <li><a href="#related-works">Related Works</a></li> <li><a href="#looking-forward">Looking Forward</a></li> <li><a href="#references">References</a></li> <li><a href="#footnotes">Footnotes</a></li> </ul> <h4 id="relu-intuition">ReLU Intuition</h4> <p>When I was first started studying machine learning I was surprised to discover that ReLU, despite its simplicity, worked well across so many architectures. I remember thinking that its asymmetry about the y-axis ran counterintuitive to what I thought a nonlinearity should look like. To this day, ReLU‚Äôs success is still not widely understood or agreed upon.</p> <p>There are some solid hypotheses at least partially explaining why ReLU works well‚Äìsuch as high computational efficiency, sparsity, mitigating the vanishing gradient problem, and biological plausibility. However, there are also certain <em>bad</em> characteristics, such as the dying ReLU, whose impact has not seemed to stop ReLU‚Äôs success. Today I urge you to ask a question‚Äìwhat if that ReLU dying wasn‚Äôt so bad after all? <em>What if your network didn‚Äôt need that neuron?</em></p> <h4 id="neuroplasticity">Neuroplasticity</h4> <p>The human brain does an amazing job at adapting to new information, experiences, and environments, a capability known as neuroplasticity [<a href="#1">1</a>]. This remarkable feature allows the brain to reorganize itself by forming new neural connections throughout life, enabling learning, memory development, and recovery from brain injuries. In response to experience, neurons in the brain form stronger connections, or synapses, when they are frequently used, embodying the ‚Äúuse it or lose it‚Äù principle. This strengthening, known as synaptic plasticity, is fundamental to learning and memory. Conversely, synapses weaken or are eliminated when they are seldom used, optimizing brain efficiency by removing less useful connections. This dynamic process ensures that our brains are continually shaped and reshaped by our experiences and interactions with the world.</p> <p>So we know that the human brain does a great job at learning and adapting due to neuroplasticity‚Äîhow does deep learning leverage neuroplasticity?</p> <p>In Artifical Neural Networks (ANNs) the connections between neurons can strengthen or weaken, similar to how synapses strengthen or weaken within the human brain. This is the crux of the ‚Äúlearning‚Äù in deep learning. However, there is one aspect of neuroplasticity within the brain that is not captured very well within deep learning, which is structural neuroplasticity. Structural neuroplasticity refers to the brain‚Äôs ability to physically change its structure over time. In the framework of deep learning, this is not generally possible due to the inherent layer structure of ANNs. Rather, in the case of <em>vanilla</em> fully connected layers, all neurons in a layer are connected to all neurons in the subsequent layer‚Äîbut are not directly connected to any layers before or after that layer and cannot create or remove connections.</p> <h4 id="the-neuroplasticity-hypothesis">The Neuroplasticity Hypothesis</h4> <p>The Neuroplasticity Hypothesis posits that the success of several architectural components within deep learning, such as ReLU as well as Residual Connections, can be attributed to their ability to approximate structural neuroplasticity in the brain.</p> <p>As discussed earlier, using ReLU can cause the dying ReLU ‚Äúissue‚Äù. However, in the case of structural neuroplasticity, this ‚Äúissue‚Äù begins to look like an advantage. Particularly, using ReLU allows for the simulation of neurons effectively being removed from a model, simulating useless synapses being removed in the brain. Thus, if a neuron is not necessary for solving a problem, and it constantly receives a negative gradient, it being dead can help ensure a neural network does not use it! With traditional nonlinearities such as sigmoid or tanh this characteristic would be more challenging or impossible to approximate.</p> <h4 id="residual-connections-and-densenet">Residual Connections and Densenet</h4> <p>Residual Connections [<a href="#4">4</a>] <em>somewhat<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></em> simulate neurons being able to connect to other layers. If this connection ends up being useful, it will be optimized for, resulting in neurons that leverage it. However, if hurtful, residual connections will optimize to avoid using the intermediate layer and approximate the identity function. This allows neural network‚Äôs behavior to adapt to simulate including one layer rather than two where the residual connection exists. Therefore, both of these characteristics approximate some aspects of structural neuroplasticity, giving another unique perspective on why Residual Connections work!</p> <p>Densenet [<a href="#5">5</a>] can be seen as similar to Residual Connections‚Äîjust without the ability to approximate the identity function. Particularly, the characteristic feature of Densenets having layers concatenated to every other future layer can be seen as similar to neurons being able to connect to any arbitrary neuron. Therefore, this also simulates some portions of structural neuroplasticity.</p> <h2 id="related-works">Related Works</h2> <p>Neural Architecture Search (NAS) [<a href="#2">2</a>] is perhaps the most related work to this hypothesis within deep learning. So why has NAS been less popular in recent years?</p> <p>Honestly, I don‚Äôt have the best response to this as I have not extensively studied NAS and was not in the field when it was popular. However, my primary hypothesis is the nature of most NAS algorithms requiring bilevel optimization or having a discrete set of candidate architectures.</p> <p>Ideally, rather than having an outer loop and trying things, either through gradient descent as in DARTS [<a href="#3">3</a>], or via complex search algorithms over a discrete state space, NAS would work dynamically within a single training loop over a single model. That is, the layers/structures helpful for performance would dynamically be added or removed.</p> <p>This is somewhat simulated in the one-shot model [<a href="#6">6</a>], where a single model is trained that contains all operations. However, this still has a limitation in that it generally drops an entire operation over a layer or a whole layer at once. Ideally, operations over singular neurons would be able to be learned during training as they are during the constant adaptation of the human brain.</p> <h2 id="looking-forward">Looking Forward</h2> <p>The first iductive bias within deep learning that I see being explored due to this hypothesis is the natural layer structure of deep neural networks. Particularly, one could try to train models without layers, but rather, neurons being able to connect to any other neuron. This would ultimately transform artificial neural networks from having a sequential layer based structure to having a graph based structure‚Äîbetter simulating neuroplasticity within the human brain.</p> <p>The biggest issue with taking this step immediately is computational efficiency. Conducting a forward pass given nodes of a graph with modern hardware is much more expensive than conducting a forward pass with sequential layers when the number of graph nodes is significantly higher than the number of layers. Additionally, this would require a differentiable algorithm to dynamically create and remove connections (synapses) between arbitrary neurons.</p> <p>Long-term, I expect architecture/techniques that further simulate structural neuroplasticity within deep learning to become more and more popular for the same reasons as the architectures/components mentioned above. This could look like more learnable parameters as well as removing more inductive biases that limit structural neuroplasticity.</p> <h2 id="references">References</h2> <p><a name="1"></a>[1] Puderbaugh, Matt, and Prabhu D. Emmady. ‚ÄúNeuroplasticity.‚Äù In StatPearls [Internet]. StatPearls Publishing, 2023.</p> <p><a name="2"></a>[2] Weng, Lilian. (Aug 2020). Neural architecture search. Lil‚ÄôLog. https://lilianweng.github.io/posts/2020-08-06-nas/.</p> <p><a name="3"></a>[3] Liu, Hanxiao, Karen Simonyan, and Yiming Yang. ‚ÄúDarts: Differentiable architecture search.‚Äù arXiv preprint arXiv:1806.09055 (2018).</p> <p><a name="4"></a>[4] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ‚ÄúDeep residual learning for image recognition.‚Äù In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016.</p> <p><a name="5"></a>[5] Huang, Gao, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. ‚ÄúDensely connected convolutional networks.‚Äù In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708. 2017.</p> <p><a name="6"></a>[6] Bender, Gabriel, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. ‚ÄúUnderstanding and simplifying one-shot architecture search.‚Äù In International conference on machine learning, pp. 550-559. PMLR, 2018.</p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>I say Residual Connections <em>somewhat</em> simulate neurons being able to connect to future layers because they can be seen as being connected to a future layer through an identity matrix (resulting in the summation operation). However, in the standard definition of layers being connected involving a learnable parmeter matrix for the weight or <em>synaptic connection</em> between each neuron, they are not connected.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="AI,"/><category term="neuroscience"/><category term="deep_learning,"/><category term="neuroplasticity,"/><category term="ReLU,"/><category term="residual_connections"/><summary type="html"><![CDATA[What do many of the most impactful deep learning architectural components have in common?]]></summary></entry><entry><title type="html">Does Biological Intelligence Have Any Advantages Over Digital Intelligence?</title><link href="https://alexiglad.github.io/blog/2023/biological_intelligence/" rel="alternate" type="text/html" title="Does Biological Intelligence Have Any Advantages Over Digital Intelligence?"/><published>2023-10-04T00:00:00+00:00</published><updated>2023-10-04T00:00:00+00:00</updated><id>https://alexiglad.github.io/blog/2023/biological_intelligence</id><content type="html" xml:base="https://alexiglad.github.io/blog/2023/biological_intelligence/"><![CDATA[<p>I found Geoffrey Hinton‚Äôs <a href="https://www.youtube.com/watch?app=desktop&amp;v=rGgGOccMEiY&amp;ab_channel=CSERCambridge">‚ÄúTwo Paths to Intelligence‚Äù</a> talk particularly fascinating. In addition to his strong sense of humor, despite discussing the possible end of the world, I was thoroughly impressed by Dr. Hinton‚Äôs ability to reason about the differences in biological and digital intelligence.</p> <p>One point stuck out to me the most‚Äìparticularly his point that digital intelligence can be shared rapidly. Currently, it takes less than 30 seconds to upload a mini (not quite as capable as humans yet) digital intelligence using websites such as Github. Once uploaded, almost anyone on the internet can access, download, and use this digital intelligence. This simplified weight sharing takes advantage of the <a href="https://www.hoppersroppers.org/fundamentals/Hardware/2-ComputersareDeterministic.html">deterministic nature</a> of computers for a given instruction set.</p> <p>In contrast, humans cannot currently upload our knowledge to a server or share our ‚Äòweights‚Äô. As such, the transfer of biological intelligence occurs through the slow process of communication-based knowledge distillation, where around 39-60 bits of information can be transferred per second [<a href="#1">1</a>][<a href="#2">2</a>]. This rate of information transfer in biological intelligence is significantly less than the rate of information transfer in digital intelligence‚Äìwhere billions or more bits per second can be shared due to the deterministic nature of computers operating on specific instruction sets. This constraint poses a huge restraint on the spread of human intelligence. <em>Imagine if as children information from the encyclopedia and all scientific papers was immediately uploaded to our brains.</em></p> <p>Despite the information transfer bottleneck imposed by knowledge distillation, I believe there is a key benefit of this slow knowledge transfer that I will argue has helped make humans successful as a species: creativity.</p> <p>The gradual accumulation of knowledge in humans begins in early childhood. From the moment of birth, our internal models of the world start to take shape. As we reach developmental milestones, where our understanding of the world has achieved a general comprehension level, a more structured form of learning is introduced. Paricularly, in educational settings, a curriculum incrementally exposes students to various subjects, methodically building upon prior knowledge.</p> <p>This slow and structured learning approach in humans stands in stark contrast to the learning mechanisms in digital intelligence‚Äìwhere AI systems often undergo a different training process. Except in cases of <a href="https://ai.stackexchange.com/questions/40241/what-is-curriculum-learning-in-reinforcement-learning#:~:text=Curriculum%20learning%20is%20a%20training,of%20tasks%20or%20training%20samples.">Curriculum Learning</a>, where AI is taught using a gradual increase in the complexity of training samples to mimic human learning, these systems are usually exposed to an entire training dataset at once with varying levels of difficulty. This method allows them to absorb large amounts of information non-sequentially, contrasting with the human method of progressively building knowledge upon itself.</p> <p>Although these characteristics of digital intelligence may sound beneficial, I believe the slow and organic development of knowledge in biological intelligence has distinct advantages. It allows for diverse interpretations and intuitions to flourish‚Äìlaying the groundwork for creativity. In biological intelligence, each individual develops their understanding and internal representations of information over time through personal experiences and learning. This process results in unique interpretations and perspectives, essential for the development of new ideas.</p> <p>For example, in learning a specific subject, people‚Äôs grasp and internalization of concepts can vary significantly. Some may quickly and intuitively understand the material, while others might need more time and structured guidance to achieve the same level of comprehension. This variance can be attributed to different internal representations of prior knowledge. While this diversity in understanding and representation may present challenges in teaching, I believe that it is precisely this diversity in addition to the slow building of an intuition through curriculum learning that breeds creativity. Particularly, this diversity results in a rich landscape of perspectives, where each person‚Äôs intuition, shaped by their unique experiences and knowledge, guides them towards exploring different ideas and solutions‚Äìeventually leading to what we call innovation.</p> <p>If we accept this viewpoint that diversity in beliefs and representation as well as slow curriculum learning are all potentially responsible for human creativity, some questions arise:</p> <ul> <li>Is human hardware being non-uniform, or its potential non-determinism<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, an advantage in innovative capabilities?</li> <li>Does the diversity and complexity of biological neural hardware in humans play a crucial role in fostering creativity, suggesting that modern computers, with their current deterministic hardware architectures, may inherently lack the capability to exhibit creativity in the same manner as humans?</li> <li>How can we increase the creative capabilities of digital intelligences? Is curriculum learning, being similar to how humans learn, a possible solution? Is reinforcement learning a solution to increasing creativity, as stated by people such as <a href="https://www.youtube.com/watch?v=OPZxs6IXH00&amp;t=850&amp;ab_channel=AlignmentWorkshop">Ilya Sutskever</a> and <a href="https://www.youtube.com/watch?v=17NrtKHdPDw&amp;list=WL&amp;index=2&amp;t=42s&amp;ab_channel=RAIL">Sergey Levine</a>.</li> </ul> <p>I hope this blog sparked some thoughts regarding what differences currently exist between biological and digital intelligence‚Äìand how this impacts characteristics of intelligence such as creativity! It‚Äôs also important to note that the definition of digital intelligence and associated training approaches will most definitely change over the coming years/decades!</p> <h2 id="updates">Updates</h2> <p>This idea is further supported by a work comparing the capabilities of AI and children with regards to innovation [<a href="#3">3</a>]. This work found that children were able to find novel causal relationships, or innovate, better than the best of today‚Äôs Large Language Models (LLMs). In contrast, they found that modern LLMs excel at tasks involving imitation‚Äìrelying on their large pretraining corpus. Therefore, this work further supports the idea that modern digital intelligences do not have the same creative abilities that humans do.</p> <h2 id="references">References</h2> <p><a name="1"></a>[1] Christophe Coup√© et al. , ‚ÄúDifferent languages, similar encoding efficiency: Comparable information rates across the human communicative niche.‚Äù Sci. Adv. 5, eaaw2594 (2019). DOI: 10.1126/sciadv.aaw2594</p> <p><a name="2"></a>[2] Reed, Charlotte M., and Nathaniel I. Durlach. ‚ÄúNote on information transfer rates in human communication.‚Äù Presence 7, no. 5 (1998): 509-518.</p> <p><a name="3"></a>[3] Yiu, Eunice, Eliza Kosoy, and Alison Gopnik. ‚ÄúImitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?.‚Äù arXiv preprint arXiv:2305.07666 (2023).</p> <p><em>This blog was developed with the assistance of ChatGPT.</em></p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>I believe that humans do not impose randomness onto the universe and therefore are completely deterministic (<a href="https://en.wikipedia.org/wiki/Hard_determinism">hard-determinism</a>). However, the question of humans imposing randomness versus determinism remains a subject of ongoing philosophical debate.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="AI"/><category term="intelligence,"/><category term="creativity,"/><category term="diversity,"/><category term="curriculum_learning"/><summary type="html"><![CDATA[How does creativity come about?]]></summary></entry></feed>