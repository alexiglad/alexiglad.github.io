---
layout: post
title: "Energy-Based Transformers are Scalable Learners and Thinkers"
date: 2025-06-01
description: What are Energy-Based Transformers and why should I care about them?
tags: deep learning, ai, reasoning, system 2 thinking, scaling, energy based models, energy-based models, transformers, ebms, verification, scaling law, test-time compute, inference-time compute, cognitively inspired energy based world models
giscus_comments: true
categories: AI, neuroscience
---


[Paper: https://energy-based-transformers.github.io/static/pdfs/paper.pdf](https://energy-based-transformers.github.io/static/pdfs/paper.pdf)[^1]

**TLDR**: We **outscale** (feed-forward) transformers while **generalizing** reasoning/system 2 thinking to any modality/problem **without** requiring verifiable rewardsðŸ˜®.

<img src="/assets/img/blog/ebt/model_comparison.png" alt="Comparison of EBTs to Existing Autoregressive Approaches" width="1000" />


<!-- This is inspired by several facets of human cognition, such as System 2 thinking from psychology (which is prolonged, deliberate, and dynamic thinking). Existing AR approaches cannot achieve all cognitive facets discussed in the paper while EBWM can. -->

<!-- **EBWM scaled better than traditional autoregressive transformers** in data efficiency and GPU hours at sequence modeling in CV. -->


*This blog is written for readers not familiar with all topics discussed, and therefore makes simplifications when describing various topics. Feel free to skip certain sections that are review for you.*




## How can we Generalize Reasoning/System 2 Thinking?
Current approaches for reasoning/System 2 Thinking (I'll use the term System 2 from now on out just to simplify) in AI generally rely on **verifiable rewards**, or rewards that cannot be hacked under any circumstance and that can be easily evaluated. An example of a verifiable reward is a solution to a math problem, where we know the answer we want the model to give (i.e., we know the answer to 5 + 5 = 10), and therefore we can check the output of the model being equal to 10.

*Seems cool right, so what's wrong with this approach?*

Well, first off, the approach relies on the problem being easily verifiable (checking the answer is correct easily). Many problems do not take this form, such as creative writing, which is an inherently subjective domain and therefore not easily verifiable. As humans we can easily think over complex problems/domains such as creative writing, relationships, career choices, and ideation. Second, existing approaches only really scale well at thoughts performed over text, *we want thinking over any modality![^2]* Lastly, and most importantly, existing approaches rely on human supervision to give rewards. Us humans (and our close animal relatives), were able to learn how to think and reason without any supervision---so a truly intelligent AI should be able to do the same!


<!-- In fact, as humans, we can think/reason about math (a verifiable domain), but we can also think about so many other complex problems/domains such as creative writing, relationships, career choices, and ideation---*so why shouldn't machines be able to do the same thing?* That is, why should we develop thinking approaches such that they only work in a small subset of domains when us humans can think over any problem? I'd argue that approaches with such limited scope are inherently unscalable towards the generality of human-level intelligence. -->

So, in an effort to generalize the System 2 capablities of current models, we ask the most important research question of the paper, which is: **"Can we rely entirely on unsupervised learning to develop System 2 Thinking?"** Relying on unsupervised learning to develop System 2 Thinking would enable models to think on any problem/modality, without relying on any human supervision, just like us humans do![^3]

So this is what we chased after in the paper. Before we can achieve System 2, we first need to know what capabilities are necessary for reasoning/System 2. In the paper we identify three capabilities inspired by human cognition, which are: 

1. The ability to think for longer (dynamic computation allocation)
2. The ability to express uncertainty (uncertainty in continuous state spaces)
3. The ability to verify whether predictions are correct or not (prediction verification)

While this list may not be completely comprehensive in the quest for human like thinking---these capabilities form the basis for System 2. As an intuitive example, if I asked you a question like: *"what's 57 * 63"*, you'd probably first realize that you don't immediately know the answer (uncertainty and verification), think for a longer amount of time (dynamic computation), and then possibly check (verify) your work! Alternatively, if I asked you *"what's 2 + 2"* you'd probably (hopefully :) immediately know the answer is 4, and be able to verify that answer.

Ok, now that we know what the necessary prerequisites for System 2 are, the question becomes, *"how can we learn these capabilities from unsupervised learning?"* Well, it turns out that there is a very simple and elegant solution that achieves all three of these capabilities at the exact same time. The idea is to first learn a verifier (a model that tells you the goodness of a prediction given some context), and then optimize predictions with respect to this verifier. Learning a verifier immediately solves the problems of expressing uncertainty and verifying predictions, and optimizing predictions with respect to this verifier enables dynamic computation :)

It turns out that this intuitive idea is actually the definition of Energy-Based Models (EBMs)! EBMs learn to assign a scalar **energy** value denoting the goodness/compatibility/unnormalized probability of a set of variables---which is in this case a context and prediction pair. In fact, Yann LeCun has been [saying this](https://openreview.net/pdf?id=BZ5a1r-kVsf) for a while now.

The key idea behind EBMs is that configurations with lower energy are more probable and compatible with each other, while configurations with higher energy are less likely. The goal of an EBM is to learn an energy function (which maps inputs to a scalar energy; in the case of our paper the energy function is just the entire neural network) that gives lower energy to "correct" or "desirable" configurations, like real data points, and higher energy to "incorrect" or "undesirable" ones, like noise or outliers.

For example, if the given context was a video of a red ball flying up through the air, a *high* energy continuation may be a video of a dog chewing on its toy, while a *low* energy continuation might be a red ball flying down through the air. This red ball flying down through the air is more compatible with the context, which implies lower energy.

For more information on how these EBMs are actually trained, please reference the approach section of the paper!

Ok, so the intuition of EBMs make sense---but how do they actually work in practice, does the thinking actually help performance? We conducted experiments to test this by comparing EBMs (more particularly Energy-Based Transformers or EBTs, more on that soon) against standard feed-forward Transformers on tasks such as language modeling. You can see from the left subfigure that, especially on out-of-distribution data, thinking with EBTs significantly improves performance over feed-forward Transformers. Particularly, by thinking for longer and also self-verifying EBTs can out-generalize feed-forward transformers. It's important to note that EBTs here are improving the performance of **every single next token**, and not just a final reasoning accuracy like current "reasoning" foundation models. The right subfigure also demonstrates promising results, where the performance from thinking *improves with scale*, suggesting that EBTs trained at scale will benefit even more from thinking than EBTs trained at the current scale.

<img src="/assets/img/blog/ebt/thinking_performance.png" alt="EBT Thinking Performance" width="1000" />

Another amazing result (that needs to be tested more) is the effect of System 2 Thinking on generalization to data that varies in Out-of-Distributioness (OOD). For example, below we can see a plot demonstrating that as data becomes more OOD, the performance gains from thinking increase. This aligns with results in psychology, where System 2 in humans is used to generalize to new unseen scenarios. 

<img src="/assets/img/blog/ebt/scaling_thinking_nlp_ar_ood.svg" alt="EBT Thinking Generalization on OOD Data" width="500" />




*Ok so we have Generalized System 2 Thinking, but...*
## How Come this Outscales (Feed-Forward) Transformers?



This is a good question that is not something with a definitive answer

First, let's go over the experimental setup to see how we tested this (discuss model params, EBT vs EBM decoder only, etc)

TODO add best plots from NLP and CV



We also compared EBTs to DiTs in simple image denoising tasks and achieved very promising results. For more information on how EBTs work and any details including pseudocode please feel free to reference the [paper](https://energy-based-transformers.github.io/static/pdfs/paper.pdf)!


## Conclusion and a Sprinkle of Intuition


While the results are promising, there is a long way to go in scaling these models up (I'm mainly looking at you, potential stability issues). But, I'm confident that in the next 3 years EBTs (or some variant) will be pretty common (let's check back and see:). The main reason I see EBTs being adopted, at least in the short term, is the improved generalization and data efficiency (in fact, these things go hand in hand as better generalization -> better data learning efficiency). Strong generalization is by far the most important aspect of any given model (as what else really matters besides generalization), and data efficiency has become increasingly important (see [this video](https://www.youtube.com/watch?v=6nJZopACRuQ&ab_channel=OpenAI) by the OpenAI pre-training team where they mention that the biggest blocker to AI progress is more data-efficient algorithms)! For these reasons alone I'm confident there will be high interest in EBTs, in addition to the System 2 capabilities, but we shall see the world is challenging to predict.

Generally, approaches that increase the flexibility of models scale best in the long run (i.e., see CNNs -> ViTs, statistical learning -> NNs, almost all of AI as a field in general). EBTs are just the next example of this, where (if we squint a little bit) EBTs are more flexible than DiTs, which are more flexible than standard feed-forward models such as RNNs and traditional transformers (assuming they only update with new state information, more on this nuance in the paper).


Thanks to all coauthors for all the help with this work, and I'm super excited to see what this work leads to in the future! Feel free to check the [paper](https://energy-based-transformers.github.io/static/pdfs/paper.pdf) for more information and details.

## References

See references in the [paper](https://energy-based-transformers.github.io/static/pdfs/paper.pdf).

## Footnotes

[^1]: The old version of this paper was called "Cognitively-Inspired Energy Based World Models" but because of me starting my PhD, working with other people, along some other things, we thought a rebrand was fitting. We also conducted much more thorough experiments due to having additional compute.

[^2]: While there are approaches for multimodal reasoning, these generally still have models think by outputting text. Thinking over continuous signals using RL at scale has not yet succeeded to my knowledge.

[^3]: The paper discusses other forms of System 2 in more depth, such as diffusion/RNNs.











<!-- OLDER BLOG -->

<!-- Large Language Models (LLMs) have achieved amazing feats, but still struggle to reason and plan.
So how can we train LLMs, and more broadly, any type of model to reason and plan like humans? Our new work, [Cognitively Inspired Energy-Based World Models](https://arxiv.org/abs/2406.08862), aims to address this.

To understand why current LLMs/world models cannot reason and plan at the level of humans, consider the two questions below:

What is 2 + 2?

What is 67 * 54?

These questions vary drastically in difficulty---the first you could probably solve without even deliberately thinking but the second would take deliberate mathematical reasoning. Yet, most existing world models are trained to make every prediction with a fixed amount of computational resources (If you're thinking CoT is a counterexample to this check the paper!) As humans, we naturally adjust the computational resources we use towards the difficulty of the problem, which is more broadly referred to as System 2 thinking in psychology. 

We trained world models to be able to approximate aspects of System 2 thinking, through developing 'Energy-Based World Models' (EBWM). Not only are these models able to dynamically allocate compute based on the difficulty of a given problem during inference, but they can also achieve several other facets of cognition not possible with existing traditional autoregressive approaches (e.g. Autoregressive Transformers, RNNs, Diffusion Transformers, etc).

This matters, because it provides promising steps towards training world models that can truly think, reason, and plan, at the same level as humans---a problem I'm confident is one of the largest blockers towards achieving human-level intelligence/AGI.

Before we continue, let's review what some of the topics we'll be discussing are.

## Review

### World Models

Intuitively, world models are predictive models of the world. They have learned various laws of physics, such as gravity, and know how people move, how the sun sets, and various other aspects of the world. The world models that our paper focused on are a specific instance of world models where there are several sensory inputs (i.e. multiple video frames) and the model is trained to predict the next sensory input (i.e. the next frame)[^2]. Mathematically, this is equivalent to the following:

$$
x(t+1) = F(x(1), x(2), \ldots, x(t))
$$

Where $x(1), ..., x(t+1)$ are sensory inputs (i.e. different video frames) and F is the function (or more specifically energy function) being learned. 

### Energy-Based Models

Energy-Based Models (EBMs) assign a scalar value, called the energy, to each configuration of inputs to a model. The key idea is that configurations with lower energy are more probable and compatible with each other, while those with higher energy are less likely. The goal of an EBM is to learn an energy function (which is usually just a neural network/the model) that gives lower energy to "correct" or "desirable" configurations, like real data points, and higher energy to "incorrect" or "undesirable" ones, like noise or outliers.

In the context of world models trained as EBMs, you can think of this energy as how probable/compatible the *predicted next state* is with the *context/past*. 

For example, if the given context was a video of a red ball flying up through the air, a *high* energy continuation may be a video of a dog chewing on its toy, while a *low* energy continuation might be a red ball flying down through the air. This red ball flying down through the air is more compatible with the context, which implies lower energy.

### Markov Chain Monte Carlo

EBMs trained with neural networks (NNs) have some cool characteristics. Since NNs are fully differentiable, these models can use a method called Markov Chain Monte Carlo (MCMC). Simply put, MCMC under the context of EBMs is a technique that helps models start from random noise and iteratively improve upon their predictions. It does this by minimizing the learned energy function, or by backpropagating the gradient from the energy scalar to the input being improved upon. You can think about this intuitively as asking the model, "How can I adjust this input to improve its likeliness", and the model telling you how to adjust each and every aspect of the input (i.e. each pixel in the video frame) to minimize the energy and therefore increase that input's compatibility with the other inputs.

### EBWM 

Now, let's take a look at EBWM in NLP and CV (note that we visualize words and images for NLP and CV respectively, but note that this is all done at the token/embedding level):

<!-- ![EBWM in NLP and CV](/assets/img/blog/ebwm.png) -->
<!-- <img src="/assets/img/blog/ebwm.png" alt="EBWM in NLP and CV" width="1000" />

The orange boxes here represent the context. The yellow boxes represent the predicted future state (which starts as random noise). First, EBWM predicts the energy of the initially predicted future state (which is likely to be high since it is random noise). Then, using MCMC, this prediction is iteratively improved upon. This ability to dynamically improve upon predictions, while giving an energy score for each of them, allows the models to achieve several cognitive feats described below. -->

<!-- ### Energy-Based Transformer
The Energy-Based Transformer can just be thought of as a decoder transformer with causal attention specifically made to be parallelizable for EBMs. To understand why this is necessary, please check the [paper](https://arxiv.org/abs/2406.08862).

### Implementation Details 
  
The pseudocode is really helpful for understanding how this is implemented. Feel free to skip to the next subsection if you already get it. 

Below is the pseudocode for a traditional autoregressive decoder only transformer (in CV):

```python
criterion = torch.nn.SmoothL1Loss(beta=1.0)

input_embeddings = embeddings[:, :-1]
next_embeddings = embeddings[:, 1:]

refined_embeddings = self.transformer(input_embeddings)
predicted_embeddings = self.output(refined_embeddings)

loss = criterion(predicted_embeddings, next_embeddings)
```

Below is the pseudocode for EBWM (in CV):

```python
# Pseudocode in PyTorch for loss calculation, similar to Wang Et Al. (https://arxiv.org/abs/2302.01384)

# make sure to enable gradient tracking, i.e. wrap with torch.set_grad_enabled(True)
with torch.set_grad_enabled(True):
    criterion = torch.nn.SmoothL1Loss(beta=1.0)
    predicted_embeddings = torch.randn_like(embeddings)
    # different corruption techniques can be used, described in C.2 of the main paper (https://arxiv.org/abs/2406.08862)
    next_embeddings = embeddings[:, 1:]

    for _ in range(num_mcmc_steps):
        # Detach embeddings so that no gradient flows through past steps
        predicted_embeddings = predicted_embeddings.detach()
        
        # Refine embeddings through the Energy-Based Transformer (EBT)
        refined_embeddings = self.transformer(predicted_embeddings)
        
        # Predict energies through a linear layer (energy predictor)
        predicted_energies = self.energy_predictor(refined_embeddings)
        
        # Compute the gradient of predicted energies w.r.t. predicted embeddings
        predicted_embeddings_grad = torch.autograd.grad(predicted_energies.sum(), 
                                                        predicted_embeddings, 
                                                        create_graph=True)[0]
        
        # Perform gradient descent w.r.t. the energy function where self.alpha is the learnable MCMC 'learning rate'
        predicted_embeddings = predicted_embeddings - self.alpha * predicted_embeddings_grad
        
        # Calculate reconstruction loss based on predicted and ground truth embeddings
        reconstruction_loss += criterion(predicted_embeddings, next_embeddings)


```

A similar intuition applies to NLP. The biggest differences are mapping to/from the vocabulary space. In the case of a traditional autoregressive transformer, this means mapping from the embedding space to the vocabulary space after the final transformer block (a linear layer, self.output in the pseudocode). In the case of EBWM, since predicting the next token is done in the input space rather than the output space, a mapping from the vocabulary space to the embedding space must be done before the first transformer block (again with a linear layer).  -->
  

<!-- ### So what other feats can EBWM achieve that existing autoregressive approaches can't?

The paper discusses four capabilities, which are listed below:
* Predictions shape internal state
* Evaluation of predictions
* Dynamic allocation of resources
* Modeling uncertainty in continuous state spaces -->

<!-- Traditional autoregressive transformers and RNNs cannot achieve any of these capabilities, while diffusion models can only achieve two. EBWM can achieve all four, and this motivates our pursuance of the EBWM architecture. -->

<!-- ### So what are the main technical contributions? -->

<!-- There are four main contributions in the main paper--the first is the proposal of this architecture motivated by facets of human cognition. Hopefully, by now you understand this intuition :). -->

<!-- Second, was the design of an *Energy-Based Transformer.* EBM's have failed to stay mainstream in the era of modern ML, so by developing a parallelizable transformer architecture specifically for EBM's we hope to advance the EBM paradigm. This transformer implementation was a core part of EBWM scaling faster than traditional AR transformers. -->

<!-- Third, we investigated EBWM scaling in CV and NLP. -->

<!-- The fourth, *not discussed much in the main paper* contribution, was the usage of a regularized (by reconstruction) objective rather than a contrastive objective for pre-training. There are existing papers that approached autoregressive modeling with an EBM, but they all used contrastive objectives. Contrastive objectives suffer from the curse of dimensionality, which makes regularized objectives more attractive for scaling. This is the key reason behind why EBWM actually scales, unlike other EBM approaches. -->

<!-- ### Takeaways from the Paper -->
<!-- The main takeaway I aim to convey is that EBWM offers an exciting opportunity for training models capable of System 2 thinking. The experiments show promising scaling, so I think further scaling (people with more GPUs :) and more investigations of the capablities after further scaling could be very exciting! -->