<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> EBWM: Cognitively Inspired Energy-Based World Models | Alexi Gladstone </title> <meta name="author" content="Alexi Gladstone"> <meta name="description" content="What are Energy-Based World Models and why should I care about them?"> <meta name="keywords" content="ai, artificial intelligence, ml, machine learning, alexi gladstone, philosophical, meaning of life"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%99%83&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alexiglad.github.io/blog/2024/EBWM/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Alexi</span> Gladstone </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/services/">services </a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/Alexi_Gladstone_CV.pdf" target="_blank" rel="noopener noreferrer"> cv <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">EBWM: Cognitively Inspired Energy-Based World Models</h1> <p class="post-meta"> Created in June 16, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep_learning,</a>   <a href="/blog/tag/world-models"> <i class="fa-solid fa-hashtag fa-sm"></i> world_models,</a>   <a href="/blog/tag/energy-based-models"> <i class="fa-solid fa-hashtag fa-sm"></i> energy-based_models,</a>   <a href="/blog/tag/autoregressive"> <i class="fa-solid fa-hashtag fa-sm"></i> autoregressive,</a>   <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers,</a>   <a href="/blog/tag/cognition"> <i class="fa-solid fa-hashtag fa-sm"></i> cognition,</a>   <a href="/blog/tag/system-2"> <i class="fa-solid fa-hashtag fa-sm"></i> system_2</a>   ·   <a href="/blog/category/ai"> <i class="fa-solid fa-tag fa-sm"></i> AI,</a>   <a href="/blog/category/neuroscience"> <i class="fa-solid fa-tag fa-sm"></i> neuroscience</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><a href="https://arxiv.org/abs/2406.08862" rel="external nofollow noopener" target="_blank">Paper: https://arxiv.org/abs/2406.08862</a></p> <h2 id="tldr">TLDR:</h2> <p>We developed a new approach for Autoregressive (AR)/world modeling<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> called Energy-Based World Models (EBWM).</p> <p><img src="/assets/img/blog/ebwm_comparison.png" alt="Comparison of EBWM to Existing Autoregressive Approaches" width="1000"></p> <p>This is inspired by several facets of human cognition, such as System 2 thinking from psychology (which is prolonged, deliberate, and dynamic thinking). Existing AR approaches cannot achieve all cognitive facets discussed in the paper while EBWM can.</p> <p><strong>EBWM scaled better than traditional autoregressive transformers</strong> in data efficiency and GPU hours at sequence modeling in CV.</p> <hr> <p><em>This blog is written for readers not familiar with all topics discussed, and therefore makes simplifications when describing various topics. Feel free to skip certain sections that are review for you.</em></p> <h3 id="problems-with-current-models">Problems with Current Models</h3> <p>Large Language Models (LLMs) have achieved amazing feats, but still struggle to reason and plan. So how can we train LLMs, and more broadly, world models to reason and plan like humans? Our new work, <a href="https://arxiv.org/abs/2406.08862" rel="external nofollow noopener" target="_blank">Cognitively Inspired Energy-Based World Models</a>, aims to address this.</p> <p>To understand why current LLMs/world models cannot reason and plan at the level of humans, consider the two questions below:</p> <p>What is 2 + 2?</p> <p>What is 67 * 54?</p> <p>These questions vary drastically in difficulty—the first you could probably solve without even deliberately thinking but the second would take deliberate mathematical reasoning. Yet, most existing world models are trained to make every prediction with a fixed amount of computational resources (If you’re thinking CoT is a counterexample to this check the paper!) As humans, we naturally adjust the computational resources we use towards the difficulty of the problem, which is more broadly referred to as System 2 thinking in psychology.</p> <p>We trained world models to be able to approximate aspects of System 2 thinking, through developing ‘Energy-Based World Models’ (EBWM). Not only are these models able to dynamically allocate compute based on the difficulty of a given problem during inference, but they can also achieve several other facets of cognition not possible with existing traditional autoregressive approaches (e.g. Autoregressive Transformers, RNNs, Diffusion Transformers, etc).</p> <p>This matters, because it provides promising steps towards training world models that can truly think, reason, and plan, at the same level as humans—a problem I’m confident is one of the largest blockers towards achieving human-level intelligence/AGI.</p> <p>Before we continue, let’s review what some of the topics we’ll be discussing are.</p> <h2 id="review">Review</h2> <h3 id="world-models">World Models</h3> <p>Intuitively, world models are predictive models of the world. They have learned various laws of physics, such as gravity, and know how people move, how the sun sets, and various other aspects of the world. The world models that our paper focused on are a specific instance of world models where there are several sensory inputs (i.e. multiple video frames) and the model is trained to predict the next sensory input (i.e. the next frame)<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Mathematically, this is equivalent to the following:</p> \[x(t+1) = F(x(1), x(2), \ldots, x(t))\] <p>Where $x(1), …, x(t+1)$ are sensory inputs (i.e. different video frames) and F is the function (or more specifically energy function) being learned.</p> <h3 id="energy-based-models">Energy-Based Models</h3> <p>Energy-Based Models (EBMs) assign a scalar value, called the energy, to each configuration of inputs to a model. The key idea is that configurations with lower energy are more probable and compatible with each other, while those with higher energy are less likely. The goal of an EBM is to learn an energy function (which is usually just a neural network/the model) that gives lower energy to “correct” or “desirable” configurations, like real data points, and higher energy to “incorrect” or “undesirable” ones, like noise or outliers.</p> <p>In the context of world models trained as EBMs, you can think of this energy as how probable/compatible the <em>predicted next state</em> is with the <em>context/past</em>.</p> <p>For example, if the given context was a video of a red ball flying up through the air, a <em>high</em> energy continuation may be a video of a dog chewing on its toy, while a <em>low</em> energy continuation might be a red ball flying down through the air. This red ball flying down through the air is more compatible with the context, which implies lower energy.</p> <h3 id="markov-chain-monte-carlo">Markov Chain Monte Carlo</h3> <p>EBMs trained with neural networks (NNs) have some cool characteristics. Since NNs are fully differentiable, these models can use a method called Markov Chain Monte Carlo (MCMC). Simply put, MCMC under the context of EBMs is a technique that helps models start from random noise and iteratively improve upon their predictions. It does this by minimizing the learned energy function, or by backpropagating the gradient from the energy scalar to the input being improved upon. You can think about this intuitively as asking the model, “How can I adjust this input to improve its likeliness”, and the model telling you how to adjust each and every aspect of the input (i.e. each pixel in the video frame) to minimize the energy and therefore increase that input’s compatibility with the other inputs.</p> <h3 id="ebwm">EBWM</h3> <p>Now, let’s take a look at EBWM in NLP and CV (note that we visualize words and images for NLP and CV respectively, but note that this is all done at the token/embedding level):</p> <p><img src="/assets/img/blog/ebwm.png" alt="EBWM in NLP and CV" width="1000"></p> <p>The orange boxes here represent the context. The yellow boxes represent the predicted future state (which starts as random noise). First, EBWM predicts the energy of the initially predicted future state (which is likely to be high since it is random noise). Then, using MCMC, this prediction is iteratively improved upon. This ability to dynamically improve upon predictions, while giving an energy score for each of them, allows the models to achieve several cognitive feats described below.</p> <h3 id="energy-based-transformer">Energy-Based Transformer</h3> <p>The Energy-Based Transformer can just be thought of as a decoder transformer with causal attention specifically made to be parallelizable for EBMs. To understand why this is necessary, please check the <a href="https://arxiv.org/abs/2406.08862" rel="external nofollow noopener" target="_blank">paper</a>.</p> <h3 id="implementation-details">Implementation Details</h3> <p>The pseudocode is really helpful for understanding how this is implemented. Feel free to skip to the next subsection if you already get it.</p> <p>Below is the pseudocode for a traditional autoregressive decoder only transformer (in CV):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">SmoothL1Loss</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="n">input_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">next_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>

<span class="n">refined_embeddings</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer</span><span class="p">(</span><span class="n">input_embeddings</span><span class="p">)</span>
<span class="n">predicted_embeddings</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">output</span><span class="p">(</span><span class="n">refined_embeddings</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">predicted_embeddings</span><span class="p">,</span> <span class="n">next_embeddings</span><span class="p">)</span>
</code></pre></div></div> <p>Below is the pseudocode for EBWM (in CV):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pseudocode in PyTorch for loss calculation, similar to Wang Et Al. (https://arxiv.org/abs/2302.01384)
</span>
<span class="c1"># make sure to enable gradient tracking, i.e. wrap with torch.set_grad_enabled(True)
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">set_grad_enabled</span><span class="p">(</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">SmoothL1Loss</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">predicted_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
    <span class="c1"># different corruption techniques can be used, described in C.2 of the main paper (https://arxiv.org/abs/2406.08862)
</span>    <span class="n">next_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_mcmc_steps</span><span class="p">):</span>
        <span class="c1"># Detach embeddings so that no gradient flows through past steps
</span>        <span class="n">predicted_embeddings</span> <span class="o">=</span> <span class="n">predicted_embeddings</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>
        
        <span class="c1"># Refine embeddings through the Energy-Based Transformer (EBT)
</span>        <span class="n">refined_embeddings</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer</span><span class="p">(</span><span class="n">predicted_embeddings</span><span class="p">)</span>
        
        <span class="c1"># Predict energies through a linear layer (energy predictor)
</span>        <span class="n">predicted_energies</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">energy_predictor</span><span class="p">(</span><span class="n">refined_embeddings</span><span class="p">)</span>
        
        <span class="c1"># Compute the gradient of predicted energies w.r.t. predicted embeddings
</span>        <span class="n">predicted_embeddings_grad</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">predicted_energies</span><span class="p">.</span><span class="nf">sum</span><span class="p">(),</span> 
                                                        <span class="n">predicted_embeddings</span><span class="p">,</span> 
                                                        <span class="n">create_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Perform gradient descent w.r.t. the energy function where self.alpha is the learnable MCMC 'learning rate'
</span>        <span class="n">predicted_embeddings</span> <span class="o">=</span> <span class="n">predicted_embeddings</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">predicted_embeddings_grad</span>
        
        <span class="c1"># Calculate reconstruction loss based on predicted and ground truth embeddings
</span>        <span class="n">reconstruction_loss</span> <span class="o">+=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">predicted_embeddings</span><span class="p">,</span> <span class="n">next_embeddings</span><span class="p">)</span>


</code></pre></div></div> <p>A similar intuition applies to NLP. The biggest differences are mapping to/from the vocabulary space. In the case of a traditional autoregressive transformer, this means mapping from the embedding space to the vocabulary space after the final transformer block (a linear layer, self.output in the pseudocode). In the case of EBWM, since predicting the next token is done in the input space rather than the output space, a mapping from the vocabulary space to the embedding space must be done before the first transformer block (again with a linear layer).</p> <h3 id="so-what-other-feats-can-ebwm-achieve-that-existing-autoregressive-approaches-cant">So what other feats can EBWM achieve that existing autoregressive approaches can’t?</h3> <p>The paper discusses four capabilities, which are listed below:</p> <ul> <li>Predictions shape internal state</li> <li>Evaluation of predictions</li> <li>Dynamic allocation of resources</li> <li>Modeling uncertainty in continuous state spaces</li> </ul> <p>Traditional autoregressive transformers and RNNs cannot achieve any of these capabilities, while diffusion models can only achieve two. EBWM can achieve all four, and this motivates our pursuance of the EBWM architecture.</p> <h3 id="so-what-are-the-main-technical-contributions">So what are the main technical contributions?</h3> <p>There are four main contributions in the main paper–the first is the proposal of this architecture motivated by facets of human cognition. Hopefully, by now you understand this intuition :).</p> <p>Second, was the design of an <em>Energy-Based Transformer.</em> EBM’s have failed to stay mainstream in the era of modern ML, so by developing a parallelizable transformer architecture specifically for EBM’s we hope to advance the EBM paradigm. This transformer implementation was a core part of EBWM scaling faster than traditional AR transformers.</p> <p>Third, we investigated EBWM scaling in CV and NLP.</p> <p>The fourth, <em>not discussed much in the main paper</em> contribution, was the usage of a regularized (by reconstruction) objective rather than a contrastive objective for pre-training. There are existing papers that approached autoregressive modeling with an EBM, but they all used contrastive objectives. Contrastive objectives suffer from the curse of dimensionality, which makes regularized objectives more attractive for scaling. This is the key reason behind why EBWM actually scales, unlike other EBM approaches.</p> <h3 id="takeaways-from-the-paper">Takeaways from the Paper</h3> <p>The main takeaway I aim to convey is that EBWM offers an exciting opportunity for training models capable of System 2 thinking. The experiments show promising scaling, so I think further scaling (people with more GPUs :) and more investigations of the capablities after further scaling could be very exciting!</p> <p>Thanks to all coauthors for all the help with this work, and I’m super excited to see what this work leads to in the future!</p> <h2 id="references">References</h2> <p>See references in the paper.</p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>I will use the term world modeling in this blog to broadly refer to all autoregressive models/LLMs. See the paper for more details on why I do this. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:2"> <p>A more general formulation of world models is in the paper, where more than just sensory inputs are conditioned on when predicting the next state/input. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/intelligence_components/">The Different Components of Intelligence</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/improving_governments/">How Can We Improve Governments?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/neuroplasticity_hypothesis/">The Neuroplasticity Hypothesis</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/biological_intelligence/">Does Biological Intelligence Have Any Advantages Over Digital Intelligence?</a> </li> <p class="mb-2" style="margin-top: 1.5rem !important">Subscribe to be notified of future articles:</p> <div class="newsletter-form-container"> <form class="newsletter-form" action="https://app.loops.so/api/newsletter-form/" method="POST" style="justify-content: flex-start"> <input class="newsletter-form-input" name="newsletter-form-input" type="email" placeholder="user@example.com" required=""> <button type="submit" class="newsletter-form-button" style="justify-content: flex-start"> subscribe </button> <button type="button" class="newsletter-loading-button" style="justify-content: flex-start"> Please wait... </button> </form> <div class="newsletter-success" style="justify-content: flex-start"> <p class="newsletter-success-message">You're subscribed!</p> </div> <div class="newsletter-error" style="justify-content: flex-start"> <p class="newsletter-error-message">Oops! Something went wrong, please try again</p> </div> <button class="newsletter-back-button" type="button" onmouseout='this.style.textDecoration="none"' onmouseover='this.style.textDecoration="underline"'> ← Back </button> </div> <noscript> <style>.newsletter-form-container{display:none}</style> </noscript> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'alexiglad/alexiglad.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Alexi Gladstone. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> <script defer src="/assets/js/newsletter.js?c3d0931971ee96e9df74ba70526c3130"></script> </body> </html>