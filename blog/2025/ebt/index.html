<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Energy-Based Transformers are Scalable Learners and Thinkers | Alexi Gladstone </title> <meta name="author" content="Alexi Gladstone"> <meta name="description" content="What are Energy-Based Transformers and why should I care about them?"> <meta name="keywords" content="ai, artificial intelligence, ml, machine learning, alexi gladstone, philosophical, meaning of life"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%99%83&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alexiglad.github.io/blog/2025/ebt/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Alexi</span> Gladstone </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/services/">services </a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/Alexi_Gladstone_CV.pdf" target="_blank" rel="noopener noreferrer"> cv <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <style type="text/css">:root,html[data-theme="dark"]{--global-bg-color:#fff!important;--global-code-bg-color:#f5f5f5!important;--global-text-color:#000!important;--global-text-color-light:#666!important;--global-theme-color:#7c3aed!important;--global-hover-color:#7c3aed!important;--global-hover-text-color:#fff!important;--global-footer-bg-color:#333!important;--global-footer-text-color:#ccc!important;--global-footer-link-color:#fff!important;--global-distill-app-color:#666!important;--global-divider-color:rgba(0,0,0,0.1)!important;--global-card-bg-color:#fff!important;--global-highlight-color:#dc2626!important;--global-newsletter-bg-color:#fff!important;--global-newsletter-text-color:#000!important;--global-tip-block:#42b983!important;--global-tip-block-bg:#e2f5ec!important;--global-tip-block-text:#215d42!important;--global-tip-block-title:#359469!important;--global-warning-block:#e7c000!important;--global-warning-block-bg:#fff8d8!important;--global-warning-block-text:#6b5900!important;--global-warning-block-title:#b29400!important;--global-danger-block:#c00!important;--global-danger-block-bg:#ffe0e0!important;--global-danger-block-text:#600!important;--global-danger-block-title:#c00!important}.only-light{display:block!important}.only-dark{display:none!important}table.table-dark{background-color:#fff!important;color:#000!important}#highlight_theme_dark{display:none!important}#highlight_theme_light{display:block!important}</style> <div class="post"> <header class="post-header"> <h1 class="post-title">Energy-Based Transformers are Scalable Learners and Thinkers</h1> <p class="post-meta"> Created in June 01, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> ¬† ¬∑ ¬† <a href="/blog/tag/deep"> <i class="fa-solid fa-hashtag fa-sm"></i> deep</a> ¬† <a href="/blog/tag/learning"> <i class="fa-solid fa-hashtag fa-sm"></i> learning,</a> ¬† <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai,</a> ¬† <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> reasoning,</a> ¬† <a href="/blog/tag/system-2-thinking"> <i class="fa-solid fa-hashtag fa-sm"></i> system-2-thinking,</a> ¬† <a href="/blog/tag/scaling"> <i class="fa-solid fa-hashtag fa-sm"></i> scaling,</a> ¬† <a href="/blog/tag/energy-based-models"> <i class="fa-solid fa-hashtag fa-sm"></i> energy-based-models,</a> ¬† <a href="/blog/tag/energy-based-models"> <i class="fa-solid fa-hashtag fa-sm"></i> energy_based_models,</a> ¬† <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers,</a> ¬† <a href="/blog/tag/ebms"> <i class="fa-solid fa-hashtag fa-sm"></i> ebms,</a> ¬† <a href="/blog/tag/verification"> <i class="fa-solid fa-hashtag fa-sm"></i> verification,</a> ¬† <a href="/blog/tag/scaling-law"> <i class="fa-solid fa-hashtag fa-sm"></i> scaling-law,</a> ¬† <a href="/blog/tag/test-time-compute"> <i class="fa-solid fa-hashtag fa-sm"></i> test-time-compute,</a> ¬† <a href="/blog/tag/inference-time-compute"> <i class="fa-solid fa-hashtag fa-sm"></i> inference-time-compute,</a> ¬† <a href="/blog/tag/cognitively-inspired-energy-based-world-models"> <i class="fa-solid fa-hashtag fa-sm"></i> cognitively-inspired-energy-based-world-models</a> ¬† ¬∑ ¬† <a href="/blog/category/ai"> <i class="fa-solid fa-tag fa-sm"></i> AI,</a> ¬† <a href="/blog/category/neuroscience"> <i class="fa-solid fa-tag fa-sm"></i> neuroscience</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><a href="https://energy-based-transformers.github.io/static/pdfs/paper.pdf" rel="external nofollow noopener" target="_blank">Paper: https://energy-based-transformers.github.io/static/pdfs/paper.pdf</a><sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> <a href="https://energy-based-transformers.github.io/" rel="external nofollow noopener" target="_blank">Website: https://energy-based-transformers.github.io/</a></p> <p><strong>TLDR</strong>: We <strong>outscale</strong> (feed-forward) transformers while <strong>generalizing</strong> reasoning/system 2 thinking to any modality/problem <strong>without</strong> requiring verifiable rewardsüòÆ. Energy-Based Transformers are the <strong>first approach</strong> to outscale feed-forward transformers across modalities and with respect to several axes including data, depth, parameters, FLOPs, etc. Energy-Based Transformers can think over every single prediction (i.e. every token in language modeling) and generalize better than existing models.</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/proposed_model.png" alt="EBT Autoregressive Modeling Architecture for text and video" width="850"> <figcaption>Figure 1: <b>EBT Autoregressive Modeling Architecture for text and video</b></figcaption> </figure> <h2 id="how-can-we-generalize-reasoningsystem-2-thinking">How can we Generalize Reasoning/System 2 Thinking?</h2> <p>Current approaches for reasoning/inference-time compute/System 2 Thinking (I‚Äôll use the term System 2 from now on out just to simplify, the paper contains a strong section on why I believe System 2 is a better term than inference-time compute) in AI generally rely on <strong>verifiable rewards</strong>, or rewards that cannot be hacked under any circumstance and that can be easily evaluated. An example of a verifiable reward is a solution to a math problem, where we know the answer we want the model to give (i.e., we know the answer to 5 + 5 = 10), and therefore we can check the output of the model being equal to 10.</p> <p><em>Seems cool, right? So what are the challenges with this approach?</em></p> <p>Well, first off, the approach relies on the problem being easily verifiable (checking the answer is correct easily). Many problems do not take this form, such as creative writing, which is an inherently subjective domain and therefore not easily verifiable. As humans, we can easily think over a wide variety of tasks such as creative writing, relationships, career choices, and coming up with new ideas. Second, existing approaches only really scale well at thoughts performed over text, <em>we want thinking over any modality!<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></em> Lastly, and most importantly, existing approaches rely on human supervision to give rewards. Us humans (and our close animal relatives), were able to learn how to think and reason without any supervision‚Äîso a truly intelligent AI should be able to do the same!</p> <p>So, in an effort to generalize the System 2 capablities of current models, we ask the most important research question of the paper, which is: <strong>‚ÄúCan we rely entirely on unsupervised learning to develop System 2 Thinking?‚Äù</strong> Relying on unsupervised learning to develop System 2 Thinking would enable models to think on any problem/modality, without relying on any human supervision, just like us humans do!<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p> <p>So this is the capability we sought after in the paper. But, before we can achieve System 2, we first need to know what capabilities (referred to as Facets of cognition) are necessary for reasoning/System 2. In the paper we identify three capabilities inspired by human cognition, which are:</p> <ol> <li>The ability to think for longer (dynamic computation allocation)</li> <li>The ability to express uncertainty (uncertainty in continuous state spaces)</li> <li>The ability to verify whether predictions are correct or not (prediction verification)</li> </ol> <p>While this list may not be completely comprehensive in the quest for human like thinking‚Äîthese capabilities form the basis for System 2. As an intuitive example, if I asked you a question like: <em>‚Äúwhat‚Äôs 57 * 63‚Äù</em>, you‚Äôd probably first realize that you don‚Äôt immediately know the answer (uncertainty and verification), think for a longer amount of time (dynamic computation), and then possibly check (verify) your work! Alternatively, if I asked you <em>‚Äúwhat‚Äôs 2 + 2‚Äù</em> you‚Äôd probably (hopefully :) immediately know the answer is 4, and be able to verify that answer.</p> <p>We can broadly classify existing paradigms (for now we can focus on the autoregressive case) based on how they predict and whether they have these capabilities.</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/model_comparison.png" alt="Comparison of autoregressive EBTs to existing autoregressive approaches" width="800"> <figcaption>Figure 2: <b>Comparison of autoregressive EBTs to existing autoregressive approaches</b></figcaption> </figure> <figure class="text-center"> <img src="/assets/img/blog/ebt/architectures_cognitive_facets.png" alt="Existing autoregressive architectures and which facets of cognition they have." width="700"> <figcaption>Figure 3: <b>Existing autoregressive architectures and which facets of cognition they have.</b></figcaption> </figure> <p>Ok, now that we know what the necessary prerequisites for System 2, the question becomes, <em>‚Äúhow can we learn these capabilities from unsupervised learning?‚Äù</em> Well, it turns out that there is a very simple and elegant solution that achieves all three of these capabilities at the exact same time. The idea is to first learn a verifier (a model that tells you the goodness/compatibility of a prediction given some context), and then optimize predictions with respect to this verifier. Learning a verifier immediately solves the problems of expressing uncertainty and verifying predictions, and optimizing predictions with respect to this verifier enables dynamic computation through performing optimization for longer.</p> <p>It turns out that this intuitive idea is actually the definition of Energy-Based Models (EBMs)! EBMs learn to assign a scalar <strong>energy</strong> value (the verification) denoting the goodness/compatibility/unnormalized probability of a set of variables‚Äîwhich in this case is a context and prediction pair. </p> <p>The key idea behind EBMs is that configurations with lower energy are more probable and compatible with each other, while configurations with higher energy are less probable. More particularly, the goal of an EBM is to learn an energy function (which maps inputs to a scalar energy; in the case of our paper the energy function is just the entire neural network) that gives lower energy to ‚Äúcorrect‚Äù or ‚Äúdesirable‚Äù configurations, like real data points, and higher energy to ‚Äúincorrect‚Äù or ‚Äúundesirable‚Äù ones, like noise or outliers.</p> <p>For example, if the given context was a video of a dog running to catch a frisbee, a <em>high</em> energy continuation may be a video of a dog chewing on its toy, while a <em>low</em> energy continuation might be the dog catching the frisee. This dog catching the frisbee scenario is more compatible with the context, which implies lower energy.</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/energy_landscape_minimization.png" alt="EBT Autoregressive Language Modeling Thinking Example" width="600"> <figcaption>Figure 4: <b>EBT Autoregressive Language Modeling Thinking Example as Energy Minimization</b></figcaption> </figure> <p>‚ÄúThinking‚Äù in these EBMs can be performed by starting with an initial (random) prediction, and then optimizing this prediction by minimizing its energy through gradient descent (shown above).</p> <p>To enable high scalability, we design a specific type of EBMs combined with the Transformer architecture and with a scalable training algorithm, which we call Energy-Based Transformers (EBTs); EBTs enable high training efficiency, stability, and parallelizability. We discover several tricks for training EBTs at scale as well as for enabling System 2 to emerge during pretraining. For more information and details on how these EBTs are actually trained, please reference the approach section of the paper.</p> <p>Ok, so hopefully by now the intuition of EBTs make sense‚Äîbut how do they actually work in practice, does the thinking actually help performance? We conducted experiments to test this by comparing EBTs against standard feed-forward Transformers (we use the SOTA recipe from the <a href="https://arxiv.org/pdf/2312.00752" rel="external nofollow noopener" target="_blank">Mamba paper</a> called the Transformer++) on tasks such as language modeling. You can see from the left subfigure that thinking with EBTs significantly improves performance over feed-forward Transformers. Particularly, by thinking for longer and also self-verifying EBTs can out-generalize feed-forward transformers to Out-of Distribution (OOD) data. It‚Äôs important to note that EBTs here are improving the performance of <strong>every single next token</strong>, and <strong>not just a final reasoning accuracy</strong> like current ‚Äúreasoning‚Äù foundation models. The right subfigure also demonstrates promising results, where the performance from thinking <em>improves with scale</em>, suggesting that EBTs trained at scale will benefit even more from thinking than EBTs trained at the current smaller scale.</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/thinking_performance.png" alt="EBT Thinking Performance" width="800"> <figcaption>Figure 5: <b>EBT Thinking Performance Compared to Transformer++ and as scale increases</b></figcaption> </figure> <p>Another amazing result (that needs to be tested more) is the effect of System 2 Thinking on generalization to data that varies in Out-of-Distribution (OOD) magnitude (how far away data is from the training distribution). For example, below we can see a plot demonstrating that as data becomes more OOD, the performance gains from thinking increase. This aligns with results in psychology, where System 2 in humans is used to generalize to new unseen scenarios.</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/scaling_thinking_nlp_ar_ood.svg" alt="EBT Thinking Effect on Generalization to OOD Data" width="400"> <figcaption>Figure 6: <b>EBT Thinking Effect on Generalization to OOD Data</b></figcaption> </figure> <p><em>Ok so we have Generalized System 2 Thinking, but‚Ä¶</em></p> <h2 id="how-come-this-outscales-feed-forward-transformers">How Come this Outscales (Feed-Forward) Transformers?</h2> <p>This is a good question that does not have as definitive of an answer as generalizing reasoning, however, I can give a general intuition backed by two main reasons for why I believe this is occurring:</p> <ol> <li>Learning to verify is (generally) easier than learning to generate.<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> EBTs learn to verify (so that they can generate), whereas feed-forward models just learn to directly generate. Therefore, EBTs generalize better, and this improved generalization leads to improved scaling.</li> <li>EBTs make weaker assumptions about the data prediction process than feed-forward Transformers, while enabling higher model flexibility (predicting data by optimizing w.r.t a verifier, which can involve many forward passes, vs. feed-forward transformers which need to predict data within a single forward pass). Generally, in AI, systems that increase flexibility and decrease assumptions win out over time (there is a <a href="https://www.youtube.com/watch?v=orDKvo8h71o&amp;ab_channel=StanfordOnline" rel="external nofollow noopener" target="_blank">great talk</a> by Hyung Won Chung on this). Thus, it makes sense under this perspective that EBTs scale better.</li> </ol> <p>We conducted several scaling experiments to be as thorough as possible in determining how EBTs scale compared to feed-forward transformers. For example, in all of the experiments shown below for language modeling, we determine the scaling rate of EBTs compared to the Transformer++ by changing just a single independent variable (as is commonly done in science, but not in empirical ‚Äúscaling law‚Äù papers<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>).</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/scaling_nlp_1.png" alt="Scaling trends for EBTs vs Transformer++ (feed-forward Transformers) in Language Modeling 1" width="900"> <figcaption>Figure 7: <b>Scaling trends for EBTs vs Transformer++ (feed-forward Transformers) in Language Modeling 1</b></figcaption> </figure> <p>Remarkably, the plots demonstrate that EBTs scale up to 35% faster than feed-forward Transformers for data!! This is perhaps the most impressive result of the paper as it suggests that EBTs are 35% more data-efficient than Transformers. This essentially means that at scale, if you needed 30T tokens for a feed-forward Transformer, you‚Äôd need less than 20T for an EBT to achieve the same pretraining perplexity. Almost as impressive is that on downstream tasks, with the same pretraining perplexity, EBTs outperform the Transformer++, suggesting better generalization (these results are in the paper). Together, these results suggest that you can get significantly better downstream task performance while using less data with EBTs compared to the standard Transformer++. The results in the other two plots also demonstrate a similar out-scaling trend for EBTs compared to the Transformer++ when it comes to batch size as well as depth.</p> <p>In fact, if we zoom in a little bit into a similar plot from a scaled up experiment we see that the gap in performance between EBTs and the Transformer++ is <em>actually increasing over time!</em> (Note that this line was fit with a log function).</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/scaling_learning_nlp_ar_data_smallm2.svg" alt="Scaling for EBTs vs Transformer++ (feed-forward Transformers) in Data Scaled up and Zoomed In" width="450"> <figcaption>Figure 8: <b>Scaling for EBTs vs Transformer++ (feed-forward Transformers) in Data Scaled up and Zoomed In</b></figcaption> </figure> <p>We see similar (although less dramatic) outscaling of EBTs compared to the Transformer++ for parameter/FLOP efficiency (at the scale we tested at, EBTs still lag behind in raw y axis performance, but scale at a higher rate, and therefore would perform better than the Transformer++ asymptotically if these trends continue).</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/scaling_nlp_2.png" alt="Scaling trends for EBTs vs Transformer++ in Language Modeling 2" width="900"> <figcaption>Figure 9: <b>Scaling trends for EBTs vs Transformer++ in Language Modeling 2</b></figcaption> </figure> <p>In CV, we observe that EBTs very dramatically outscale the Transformer++ at predicting the next frame, achieving a 33% and 34% higher scaling rate for width (embedding dimension) and parameters respectively. (These trends are less consistent than the scaling trends in language modeling though.)</p> <figure class="text-center"> <img src="/assets/img/blog/ebt/scaling_video_1.png" alt="Scaling trends for EBTs vs Transformer++ in Video Modeling" width="600"> <figcaption>Figure 10: <b>Scaling trends for EBTs vs Transformer++ in Video Modeling</b></figcaption> </figure> <p>We also compared EBTs to DiTs in simple image denoising tasks and achieved very promising results (better quality with less forward passes). For more information on how EBTs work and any details including pseudocode please feel free to reference the <a href="https://energy-based-transformers.github.io/static/pdfs/paper.pdf" rel="external nofollow noopener" target="_blank">paper</a>!</p> <h2 id="conclusion-and-a-sprinkle-of-intuition">Conclusion and a Sprinkle of Intuition</h2> <p>While the results are promising, there is a long way to go in scaling these models up (I‚Äôm mainly looking at you, potential stability issues). But, I‚Äôm confident that in the next 3 years EBTs (or some variant) will be pretty common (let‚Äôs check back and see:). The main reason I see EBTs being adopted, at least in the short term, is the improved generalization and data efficiency (in fact, these things go hand in hand as better generalization -&gt; better data learning efficiency). Strong generalization is by far the most important aspect of any given model (as what else really matters besides generalization), and data efficiency has become increasingly important (see <a href="https://www.youtube.com/watch?v=6nJZopACRuQ&amp;ab_channel=OpenAI" rel="external nofollow noopener" target="_blank">this video</a> by the OpenAI pre-training team where they mention that the biggest blocker to AI progress is more data-efficient algorithms)! For these reasons alone I‚Äôm confident there will be high interest in EBTs, in addition to the System 2 capabilities, but we shall see as the world is challenging to predict.</p> <p>Generally, approaches that increase the flexibility of models scale best in the long run (i.e., see CNNs -&gt; ViTs, statistical learning -&gt; NNs, almost all of AI as a field in general). EBTs are just the next example of this, where (if we squint a little bit) EBTs are more flexible than DiTs<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>, which are more flexible than standard feed-forward models such as RNNs and traditional transformers (assuming they only update with new state information, more on this nuance in the paper).</p> <p>Thanks to all coauthors for all the help with this work, and I‚Äôm super excited to see what this work leads to in the future! Feel free to check the <a href="https://energy-based-transformers.github.io/static/pdfs/paper.pdf" rel="external nofollow noopener" target="_blank">paper</a> for more information and details/references.</p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>The old version of this paper was called ‚ÄúCognitively-Inspired Energy Based World Models or EBWM‚Äù but because of me starting my PhD, working with other people, along with some other things, we thought a rebrand was fitting. We also conducted much more thorough experiments due to having additional compute.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> <li id="fn:2"> <p>While there are approaches for multimodal reasoning, these generally still have models think by outputting text. Thinking over continuous signals using RL at scale has not yet succeeded to my knowledge.¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> <li id="fn:3"> <p>The paper discusses other forms of System 2 in more depth, such as diffusion/RNNs.¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> <li id="fn:4"> <p>The intuition section of the paper does a good job at explaining why this is the case, making connections to theoretical computer science. But, just for flavor, consider the case of a maze. What‚Äôs more likely to generalize‚Äîthe maze generator (which has to generate a solution in a single forward pass) or the maze verifier (which only has to verify the correctness of a solution in a single forward pass)?¬†<a href="#fnref:4" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> <li id="fn:5"> <p>We also do ‚Äúscaling law‚Äù runs where we vary several independent variables at once following common practice in other ML papers. However, I‚Äôd argue these experiments are much less informative than experiments where just a single independent variable is changed at a time, as these scaling law experiments generally involve changing several parameters at once (data, batch size, depth, width, etc) meaning it‚Äôs not possible to isolate which axes two different models scale better/worse compared to one another. Changing a single indenpendent variable (one axis) at a time allows us to directly measure these things‚Äîthis follows standard scientific methodology :).¬†<a href="#fnref:5" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> <li id="fn:6"> <p>We reference the reader to the <a href="https://energy-based-transformers.github.io/static/pdfs/paper.pdf" rel="external nofollow noopener" target="_blank">paper</a> section comparing diffusion and EBMs in depth for why EBMs are more flexible. The TLDR is EBMs are a generalization of diffusion and allow for estimating (unnormalized) likelihoods or verifying at every step of the thinking process, whereas diffusion models only do this implicitly after the entire denoising process.¬†<a href="#fnref:6" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> </ol> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/intelligence_components/">The Different Components of Intelligence</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/improving_governments/">How Can We Improve Governments?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/neuroplasticity_hypothesis/">The Neuroplasticity Hypothesis</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/biological_intelligence/">Does Biological Intelligence Have Any Advantages Over Digital Intelligence?</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'alexiglad/alexiglad.github.io',
        'data-repo-id': 'R_kgDOJMn35A',
        'data-category': 'Announcements',
        'data-category-id': 'DIC_kwDOJMn35M4CmZpV',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Alexi Gladstone. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>