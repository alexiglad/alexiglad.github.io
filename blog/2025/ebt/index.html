<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Energy-Based Transformers are Scalable Learners and Thinkers | Alexi Gladstone </title> <meta name="author" content="Alexi Gladstone"> <meta name="description" content="What are Energy-Based Transformers and why should I care about them?"> <meta name="keywords" content="ai, artificial intelligence, ml, machine learning, alexi gladstone, philosophical, meaning of life"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%99%83&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alexiglad.github.io/blog/2025/ebt/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Alexi</span> Gladstone </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/services/">services </a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/Alexi_Gladstone_CV.pdf" target="_blank" rel="noopener noreferrer"> cv <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Energy-Based Transformers are Scalable Learners and Thinkers</h1> <p class="post-meta"> Created in June 01, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> Â  Â· Â  <a href="/blog/tag/deep"> <i class="fa-solid fa-hashtag fa-sm"></i> deep</a> Â  <a href="/blog/tag/learning"> <i class="fa-solid fa-hashtag fa-sm"></i> learning,</a> Â  <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai,</a> Â  <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> reasoning,</a> Â  <a href="/blog/tag/system"> <i class="fa-solid fa-hashtag fa-sm"></i> system</a> Â  <a href="/blog/tag/2"> <i class="fa-solid fa-hashtag fa-sm"></i> 2</a> Â  <a href="/blog/tag/thinking"> <i class="fa-solid fa-hashtag fa-sm"></i> thinking,</a> Â  <a href="/blog/tag/scaling"> <i class="fa-solid fa-hashtag fa-sm"></i> scaling,</a> Â  <a href="/blog/tag/energy"> <i class="fa-solid fa-hashtag fa-sm"></i> energy</a> Â  <a href="/blog/tag/based"> <i class="fa-solid fa-hashtag fa-sm"></i> based</a> Â  <a href="/blog/tag/models"> <i class="fa-solid fa-hashtag fa-sm"></i> models,</a> Â  <a href="/blog/tag/energy-based"> <i class="fa-solid fa-hashtag fa-sm"></i> energy-based</a> Â  <a href="/blog/tag/models"> <i class="fa-solid fa-hashtag fa-sm"></i> models,</a> Â  <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers,</a> Â  <a href="/blog/tag/ebms"> <i class="fa-solid fa-hashtag fa-sm"></i> ebms,</a> Â  <a href="/blog/tag/verification"> <i class="fa-solid fa-hashtag fa-sm"></i> verification,</a> Â  <a href="/blog/tag/scaling"> <i class="fa-solid fa-hashtag fa-sm"></i> scaling</a> Â  <a href="/blog/tag/law"> <i class="fa-solid fa-hashtag fa-sm"></i> law,</a> Â  <a href="/blog/tag/test-time"> <i class="fa-solid fa-hashtag fa-sm"></i> test-time</a> Â  <a href="/blog/tag/compute"> <i class="fa-solid fa-hashtag fa-sm"></i> compute,</a> Â  <a href="/blog/tag/inference-time"> <i class="fa-solid fa-hashtag fa-sm"></i> inference-time</a> Â  <a href="/blog/tag/compute"> <i class="fa-solid fa-hashtag fa-sm"></i> compute,</a> Â  <a href="/blog/tag/cognitively"> <i class="fa-solid fa-hashtag fa-sm"></i> cognitively</a> Â  <a href="/blog/tag/inspired"> <i class="fa-solid fa-hashtag fa-sm"></i> inspired</a> Â  <a href="/blog/tag/energy"> <i class="fa-solid fa-hashtag fa-sm"></i> energy</a> Â  <a href="/blog/tag/based"> <i class="fa-solid fa-hashtag fa-sm"></i> based</a> Â  <a href="/blog/tag/world"> <i class="fa-solid fa-hashtag fa-sm"></i> world</a> Â  <a href="/blog/tag/models"> <i class="fa-solid fa-hashtag fa-sm"></i> models</a> Â  Â· Â  <a href="/blog/category/ai"> <i class="fa-solid fa-tag fa-sm"></i> AI,</a> Â  <a href="/blog/category/neuroscience"> <i class="fa-solid fa-tag fa-sm"></i> neuroscience</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><a href="https://energy-based-transformers.github.io/static/pdfs/paper.pdf" rel="external nofollow noopener" target="_blank">Paper: https://energy-based-transformers.github.io/static/pdfs/paper.pdf</a><sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <p><strong>TLDR</strong>: We <strong>outscale</strong> (feed-forward) transformers while <strong>generalizing</strong> reasoning/system 2 thinking to any modality/problem <strong>without</strong> requiring verifiable rewardsğŸ˜®.</p> <p><img src="/assets/img/blog/ebt/model_comparison.png" alt="Comparison of EBTs to Existing Autoregressive Approaches" width="1000"></p> <p><em>This blog is written for readers not familiar with all topics discussed, and therefore makes simplifications when describing various topics. Feel free to skip certain sections that are review for you.</em></p> <h2 id="how-can-we-generalize-reasoningsystem-2-thinking">How can we Generalize Reasoning/System 2 Thinking?</h2> <p>Current approaches for reasoning/System 2 Thinking (Iâ€™ll use the term System 2 from now on out just to simplify) in AI generally rely on <strong>verifiable rewards</strong>, or rewards that cannot be hacked under any circumstance and that can be easily evaluated. An example of a verifiable reward is a solution to a math problem, where we know the answer we want the model to give (i.e., we know the answer to 5 + 5 = 10), and therefore we can check the output of the model being equal to 10.</p> <p><em>Seems cool right, so whatâ€™s wrong with this approach?</em></p> <p>Well, first off, the approach relies on the problem being easily verifiable (checking the answer is correct easily). Many problems do not take this form, such as creative writing, which is an inherently subjective domain and therefore not easily verifiable. As humans we can easily think over complex problems/domains such as creative writing, relationships, career choices, and ideation. Second, existing approaches only really scale well at thoughts performed over text, <em>we want thinking over any modality!<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></em> Lastly, and most importantly, existing approaches rely on human supervision to give rewards. Us humans (and our close animal relatives), were able to learn how to think and reason without any supervisionâ€”so a truly intelligent AI should be able to do the same!</p> <p>So, in an effort to generalize the System 2 capablities of current models, we ask the most important research question of the paper, which is: <strong>â€œCan we rely entirely on unsupervised learning to develop System 2 Thinking?â€</strong> Relying on unsupervised learning to develop System 2 Thinking would enable models to think on any problem/modality, without relying on any human supervision, just like us humans do!<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p> <p>So this is what we chased after in the paper. Before we can achieve System 2, we first need to know what capabilities are necessary for reasoning/System 2. In the paper we identify three capabilities inspired by human cognition, which are:</p> <ol> <li>The ability to think for longer (dynamic computation allocation)</li> <li>The ability to express uncertainty (uncertainty in continuous state spaces)</li> <li>The ability to verify whether predictions are correct or not (prediction verification)</li> </ol> <p>While this list may not be completely comprehensive in the quest for human like thinkingâ€”these capabilities form the basis for System 2. As an intuitive example, if I asked you a question like: <em>â€œwhatâ€™s 57 * 63â€</em>, youâ€™d probably first realize that you donâ€™t immediately know the answer (uncertainty and verification), think for a longer amount of time (dynamic computation), and then possibly check (verify) your work! Alternatively, if I asked you <em>â€œwhatâ€™s 2 + 2â€</em> youâ€™d probably (hopefully :) immediately know the answer is 4, and be able to verify that answer.</p> <p>Ok, now that we know what the necessary prerequisites for System 2 are, the question becomes, <em>â€œhow can we learn these capabilities from unsupervised learning?â€</em> Well, it turns out that there is a very simple and elegant solution that achieves all three of these capabilities at the exact same time. The idea is to first learn a verifier (a model that tells you the goodness of a prediction given some context), and then optimize predictions with respect to this verifier. Learning a verifier immediately solves the problems of expressing uncertainty and verifying predictions, and optimizing predictions with respect to this verifier enables dynamic computation :)</p> <p>It turns out that this intuitive idea is actually the definition of Energy-Based Models (EBMs)! EBMs learn to assign a scalar <strong>energy</strong> value denoting the goodness/compatibility/unnormalized probability of a set of variablesâ€”which is in this case a context and prediction pair. In fact, Yann LeCun has been <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf" rel="external nofollow noopener" target="_blank">saying this</a> for a while now.</p> <p>The key idea behind EBMs is that configurations with lower energy are more probable and compatible with each other, while configurations with higher energy are less likely. The goal of an EBM is to learn an energy function (which maps inputs to a scalar energy; in the case of our paper the energy function is just the entire neural network) that gives lower energy to â€œcorrectâ€ or â€œdesirableâ€ configurations, like real data points, and higher energy to â€œincorrectâ€ or â€œundesirableâ€ ones, like noise or outliers.</p> <p>For example, if the given context was a video of a red ball flying up through the air, a <em>high</em> energy continuation may be a video of a dog chewing on its toy, while a <em>low</em> energy continuation might be a red ball flying down through the air. This red ball flying down through the air is more compatible with the context, which implies lower energy.</p> <p>For more information on how these EBMs are actually trained, please reference the approach section of the paper!</p> <p>Ok, so the intuition of EBMs make senseâ€”but how do they actually work in practice, does the thinking actually help performance? We conducted experiments to test this by comparing EBMs (more particularly Energy-Based Transformers or EBTs, more on that soon) against standard feed-forward Transformers on tasks such as language modeling. You can see from the left subfigure that, especially on out-of-distribution data, thinking with EBTs significantly improves performance over feed-forward Transformers. Particularly, by thinking for longer and also self-verifying EBTs can out-generalize feed-forward transformers. Itâ€™s important to note that EBTs here are improving the performance of <strong>every single next token</strong>, and not just a final reasoning accuracy like current â€œreasoningâ€ foundation models. The right subfigure also demonstrates promising results, where the performance from thinking <em>improves with scale</em>, suggesting that EBTs trained at scale will benefit even more from thinking than EBTs trained at the current scale.</p> <p><img src="/assets/img/blog/ebt/thinking_performance.png" alt="EBT Thinking Performance" width="1000"></p> <p>Another amazing result (that needs to be tested more) is the effect of System 2 Thinking on generalization to data that varies in Out-of-Distributioness (OOD). For example, below we can see a plot demonstrating that as data becomes more OOD, the performance gains from thinking increase. This aligns with results in psychology, where System 2 in humans is used to generalize to new unseen scenarios.</p> <p><img src="/assets/img/blog/ebt/scaling_thinking_nlp_ar_ood.svg" alt="EBT Thinking Generalization on OOD Data" width="500"></p> <p><em>Ok so we have Generalized System 2 Thinking, butâ€¦</em></p> <h2 id="how-come-this-outscales-feed-forward-transformers">How Come this Outscales (Feed-Forward) Transformers?</h2> <p>This is a good question that is not something with a definitive answer</p> <p>First, letâ€™s go over the experimental setup to see how we tested this (discuss model params, EBT vs EBM decoder only, etc)</p> <p>TODO add best plots from NLP and CV</p> <p>We also compared EBTs to DiTs in simple image denoising tasks and achieved very promising results. For more information on how EBTs work and any details including pseudocode please feel free to reference the <a href="https://energy-based-transformers.github.io/static/pdfs/paper.pdf" rel="external nofollow noopener" target="_blank">paper</a>!</p> <h2 id="conclusion-and-a-sprinkle-of-intuition">Conclusion and a Sprinkle of Intuition</h2> <p>While the results are promising, there is a long way to go in scaling these models up (Iâ€™m mainly looking at you, potential stability issues). But, Iâ€™m confident that in the next 3 years EBTs (or some variant) will be pretty common (letâ€™s check back and see:). The main reason I see EBTs being adopted, at least in the short term, is the improved generalization and data efficiency (in fact, these things go hand in hand as better generalization -&gt; better data learning efficiency). Strong generalization is by far the most important aspect of any given model (as what else really matters besides generalization), and data efficiency has become increasingly important (see <a href="https://www.youtube.com/watch?v=6nJZopACRuQ&amp;ab_channel=OpenAI" rel="external nofollow noopener" target="_blank">this video</a> by the OpenAI pre-training team where they mention that the biggest blocker to AI progress is more data-efficient algorithms)! For these reasons alone Iâ€™m confident there will be high interest in EBTs, in addition to the System 2 capabilities, but we shall see the world is challenging to predict.</p> <p>Generally, approaches that increase the flexibility of models scale best in the long run (i.e., see CNNs -&gt; ViTs, statistical learning -&gt; NNs, almost all of AI as a field in general). EBTs are just the next example of this, where (if we squint a little bit) EBTs are more flexible than DiTs, which are more flexible than standard feed-forward models such as RNNs and traditional transformers (assuming they only update with new state information, more on this nuance in the paper).</p> <p>Thanks to all coauthors for all the help with this work, and Iâ€™m super excited to see what this work leads to in the future! Feel free to check the <a href="https://energy-based-transformers.github.io/static/pdfs/paper.pdf" rel="external nofollow noopener" target="_blank">paper</a> for more information and details.</p> <h2 id="references">References</h2> <p>See references in the <a href="https://energy-based-transformers.github.io/static/pdfs/paper.pdf" rel="external nofollow noopener" target="_blank">paper</a>.</p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>The old version of this paper was called â€œCognitively-Inspired Energy Based World Modelsâ€ but because of me starting my PhD, working with other people, along some other things, we thought a rebrand was fitting. We also conducted much more thorough experiments due to having additional compute.Â <a href="#fnref:1" class="reversefootnote" role="doc-backlink">â†©</a></p> </li> <li id="fn:2"> <p>While there are approaches for multimodal reasoning, these generally still have models think by outputting text. Thinking over continuous signals using RL at scale has not yet succeeded to my knowledge.Â <a href="#fnref:2" class="reversefootnote" role="doc-backlink">â†©</a></p> </li> <li id="fn:3"> <p>The paper discusses other forms of System 2 in more depth, such as diffusion/RNNs.Â <a href="#fnref:3" class="reversefootnote" role="doc-backlink">â†©</a></p> </li> </ol> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/intelligence_components/">The Different Components of Intelligence</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/improving_governments/">How Can We Improve Governments?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/neuroplasticity_hypothesis/">The Neuroplasticity Hypothesis</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/biological_intelligence/">Does Biological Intelligence Have Any Advantages Over Digital Intelligence?</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'alexiglad/alexiglad.github.io',
        'data-repo-id': 'R_kgDOJMn35A',
        'data-category': 'Announcements',
        'data-category-id': 'DIC_kwDOJMn35M4CmZpV',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Alexi Gladstone. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>