<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Future of Multimodal Does not Involve Text | Alexi Gladstone </title> <meta name="author" content="Alexi Gladstone"> <meta name="description" content="What multimodal paradigm will win out?"> <meta name="keywords" content="ai, artificial intelligence, ml, machine learning, alexi gladstone, philosophical, meaning of life"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%99%83&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alexiglad.github.io/blog/2025/future_of_multimodal/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Alexi</span> Gladstone </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/services/">services </a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/Alexi_Gladstone_CV.pdf" target="_blank" rel="noopener noreferrer"> cv <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Future of Multimodal Does not Involve Text</h1> <p class="post-meta"> Created in December 29, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/multimodal"> <i class="fa-solid fa-hashtag fa-sm"></i> multimodal,</a>   <a href="/blog/tag/llms"> <i class="fa-solid fa-hashtag fa-sm"></i> llms,</a>   <a href="/blog/tag/vllms"> <i class="fa-solid fa-hashtag fa-sm"></i> vllms,</a>   <a href="/blog/tag/video"> <i class="fa-solid fa-hashtag fa-sm"></i> video,</a>   <a href="/blog/tag/audio"> <i class="fa-solid fa-hashtag fa-sm"></i> audio,</a>   <a href="/blog/tag/text"> <i class="fa-solid fa-hashtag fa-sm"></i> text</a>   ·   <a href="/blog/category/ai"> <i class="fa-solid fa-tag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>Higher-order thoughts:</strong> <em>This is a belief I’ve had for ~2 years now, that I got after working primarily in multimodal learning for ~1 year. My belief has only strengthened with time :).</em></p> <p>There’s a concept in multimodal learning called <em>cross-modal transfer</em>—the idea that models trained on multiple modalities (like text and images) could gain <em>shared benefits</em> from learning across them. Basically, learnings from one modality (say, images) would help the model better understand another modality (say, text). This has been a desired result for a while now, and recently it’s become something of an obsession at frontier labs—both for those working on large multimodal models and those chasing <a href="https://arxiv.org/pdf/2405.07987" rel="external nofollow noopener" target="_blank">The Platonic Representation Hypothesis</a>.</p> <p>This characteristic has been sought after for good reason. Achieving strong cross-modal transfer at scale would make models so much better at understanding … well everything. Us humans do this effortlessly—a child who learns the word “dog” from a picture book can instantly recognize a real dog they’ve never seen before, and vice versa.</p> <p>One great example of where cross-modal transfer would <em>really</em> benefit models is LLMs, where text-only LLMs are pretty <a href="https://arxiv.org/pdf/2310.07018" rel="external nofollow noopener" target="_blank">terrible at reasoning about the physical world</a>. Ideally, adding the capacity for LLMs to see via images would make (V)LLMs both better at seeing images (of course), but also <em>better at reasoning solely through text.</em> After all, we’d be giving models more data, and us humans benefit massively from our visual system in learning how to reason.</p> <p>Unfortunately, this phenomenon has not been observed at all. In fact, often the opposite is observed, where adding vision to base LLMs <a href="https://arxiv.org/abs/2412.03467" rel="external nofollow noopener" target="_blank">makes</a> <a href="https://arxiv.org/abs/2309.10313" rel="external nofollow noopener" target="_blank">their</a> <a href="https://arxiv.org/abs/2402.10884" rel="external nofollow noopener" target="_blank">text</a> <a href="https://arxiv.org/abs/2505.19616" rel="external nofollow noopener" target="_blank">reasoning worse</a> :O. While this can partially be attributed to catastrophic forgetting, where text-based LLMs forget some text knowledge due to training on image-text data, this result is completely sad.</p> <p><em>So why don’t current models succeed at cross-modal transfer?</em></p> <p>I’d argue the reason isn’t tied to model architecture or scale—it’s <strong>data alignment</strong>, or how well different modalities match up in their representation, structure, and content.</p> <p>To understand why, consider what we’re actually asking models to do when we train them on text and images together. Text is <em>discrete</em> and operates at a high level of abstraction—it’s pure semantics. An image, on the other hand, is <em>continuous</em> and incredibly low-level—raw pixel values encoding edges, textures, colors. These two modalities are fundamentally different in structure.</p> <p>Now, you might say “but we have tons of image-text pairs on the internet!” And that’s true—we do have <em>some</em> alignment between text and images thanks to the labeling humans have done (alt-text, captions, etc.) and the work of data labeling companies. But this alignment exists on a spectrum, and text-image pairs sit pretty low on it. The core issue is that captions are <em>very lossy</em>—they compress images down to a handful of semantic concepts while discarding almost everything else. The text caption “a dog playing in a park” throws away the dog’s breed, pose, the lighting, the texture of the grass, the spatial layout—basically all the rich, continuous information in the actual image. We’re forcing models to learn a mapping between two wildly different representations.</p> <p>To really drive this home, consider the image below:</p> <p><img src="/assets/img/blog/forest_lake.jpg" alt="water and boat with landscape"></p> <p>How would you describe this <em>exactly</em> in text? Let me try: “An aerial view of a deep teal-green water cutting between two steep mountainsides covered in dense evergreen forest. A small white boat near the center-right is moving away from the camera, leaving a long V-shaped wake that spreads across the water’s surface, creating rippled reflections of sunlight on the left side. The left cliff face is partially exposed gray rock with vegetation clinging to ledges. In the background, a thin waterfall cascades down the right mountainside. The water transitions from darker blue-green in the foreground to lighter teal where the sunlight hits the wake.”</p> <p>That’s 90+ words, and I <em>still</em> haven’t captured the exact shade of green on each tree, the precise pattern of the ripples, the shape of every rock formation, the subtle gradients in the water, and a ton of other features. A single large image encodes millions of pixels of continuous information—it takes a massive amount of text to come close to even approximate just the content, let alone the spatial/temporal structure.</p> <p>So if text and images are so poorly aligned, what modalities <em>are</em> well aligned?</p> <p>Fortunately, nature has already given us a pair of modalities that are <em>beautifully</em> aligned: <strong>video and audio</strong>. Both are continuous signals. Both are high-dimensional and contain low-level information. They share the same temporal structure—when a door slams in a video, you hear it at exactly the same moment. The alignment isn’t something humans had to create through labeling; it’s just there.</p> <p>What makes this even better is that video-audio data exists <em>for free</em> in the real world. No expensive labeling or data curation<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. You want aligned multimodal data? Just go outside and hit record, or strap a camera on a baby (I’m slightly joking with this, but not really, see <a href="https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00039/97495/SAYCam-A-Large-Longitudinal-Audiovisual-Dataset" rel="external nofollow noopener" target="_blank">SAYCam</a>).</p> <p>A great example of people exploiting this inherent alignment is <a href="https://deepmind.google/models/veo/" rel="external nofollow noopener" target="_blank">Veo 3</a>. Unlike previous video generation models that output silent videos (looking at you, Sora), Veo 3 <em>natively</em> generates synchronized audio—dialogue, sound effects, ambient noise—all from a single model. The lip sync is surprisingly good, and sounds actually match what’s happening on screen. This isn’t post-hoc audio slapped onto generated video; the model is learning from the natural alignment of video and audio in its training data. I’d bet this shared multimodality is a big part of why Veo 3 works so well, and scaling this up with better architectures will only widen the gap between video-only and video-audio models.</p> <p>If the hypothesis that data alignment is the true bottleneck for cross-modal transfer is correct, the implications for text aren’t great. Text simply doesn’t occur at scale in natural alignment with other modalities. Sure, we have audiobooks and podcasts with transcripts, but that’s a tiny fraction of the video-audio data out there. Even when text <em>is</em> aligned with other modalities, the structural mismatch remains—you’re still mapping between discrete, abstract symbols and high-dimensional very noisy continuous signals that contain much more information.</p> <p>Therefore, my prediction is that in the long-term future of multimodal AI (maybe the next 5-10 years), text takes a back seat. The most powerful multimodal systems will be built on video and audio, with text serving as an interface for human convenience rather than a core modality for learning. These changes would enable us to train large models at scale with data that is gathered completely unsupervised–just audio and video from the real world.</p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Of course, data curation pipelines aren’t strictly necessary for video-audio data, though people still often use them to improve quality. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/ebt/">Energy-Based Transformers are Scalable Learners and Thinkers</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/intelligence_components/">The Different Components of Intelligence</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/improving_governments/">How Can We Improve Governments?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/neuroplasticity_hypothesis/">The Neuroplasticity Hypothesis</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/biological_intelligence/">Does Biological Intelligence Have Any Advantages Over Digital Intelligence?</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'alexiglad/alexiglad.github.io',
        'data-repo-id': 'R_kgDOJMn35A',
        'data-category': 'Announcements',
        'data-category-id': 'DIC_kwDOJMn35M4CmZpV',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Alexi Gladstone. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>